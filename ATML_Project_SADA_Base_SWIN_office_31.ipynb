{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "collapsed_sections": [
        "wp0Ewd14AnBj",
        "41ugZ06WBPER",
        "9asqo-VoExhV",
        "VXlLEUc9IJC6",
        "k9Y3ne2WE0z1",
        "vrWNwJeBMfvA",
        "zHDAoBjnMDFa",
        "hi3LQBlkMlXi",
        "nYRNGI5MMwmb",
        "DP1jYfNVQ_iN",
        "viwb-kgBRJ_I"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip installed install datasets"
      ],
      "metadata": {
        "id": "Z75mnFTp_ztC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8caf6e9-be5f-4440-d424-d6c50b06ae74",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIbI4UQ3_rn-"
      },
      "outputs": [],
      "source": [
        "# --------------------- Imports ------------------------- #\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torchvision import datasets, transforms, models\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.models import vit_b_16\n",
        "\n",
        "# Datasets\n",
        "# from datasets import load_dataset\n",
        "import kagglehub\n",
        "\n",
        "# Extra\n",
        "import os\n",
        "from copy import deepcopy\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import timm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Office-31"
      ],
      "metadata": {
        "id": "spAb68AuAgn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Office31(domain, transform):\n",
        "    # Download and set up the dataset path\n",
        "    path = kagglehub.dataset_download(\"xixuhu/office31\")\n",
        "    path = os.path.join(path, \"Office-31\", domain)\n",
        "    return datasets.ImageFolder(root=path, transform=transform)\n",
        "\n",
        "def get_office_data_loaders(batch_size):\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "    ])\n",
        "\n",
        "    amazon_data = Office31(\"amazon\", transform)\n",
        "    dslr_data = Office31(\"dslr\", transform)\n",
        "    webcam_data = Office31(\"webcam\", transform)\n",
        "\n",
        "    # Create data loaders\n",
        "    loader_amazon = DataLoader(amazon_data, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    loader_dslr = DataLoader(dslr_data, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    loader_webcam = DataLoader(webcam_data, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "\n",
        "    return loader_amazon, loader_dslr, loader_webcam\n"
      ],
      "metadata": {
        "id": "NnwP93vyAIgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pacs"
      ],
      "metadata": {
        "id": "QXWb4KN2AlBd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-KbFM2fmAmor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Py4n_wFMBSYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Cards"
      ],
      "metadata": {
        "id": "wp0Ewd14AnBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNetFeatureExtractor(nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        super().__init__()\n",
        "        base_model = torchvision.models.resnet50(pretrained=pretrained)\n",
        "        self.conv1 = base_model.conv1\n",
        "        self.bn1 = base_model.bn1\n",
        "        self.relu = base_model.relu\n",
        "        self.maxpool = base_model.maxpool\n",
        "        self.layer1 = base_model.layer1\n",
        "        self.layer2 = base_model.layer2\n",
        "        self.layer3 = base_model.layer3\n",
        "        self.layer4 = base_model.layer4\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        f_block1 = self.layer1(x)\n",
        "        f_block2 = self.layer2(f_block1)\n",
        "        f_block3 = self.layer3(f_block2)\n",
        "        f_block4 = self.layer4(f_block3)\n",
        "\n",
        "        return f_block2, f_block3, f_block4\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class VitBasicFeatureExtractor(nn.Module):\n",
        "\n",
        "    def __init__(self, pretrained=True, layers=[4, 8, 12]):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = timm.create_model('vit_base_patch16_224', pretrained=pretrained, features_only=True)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        features = self.model(x)\n",
        "        pooled_features = []\n",
        "        for feature in features:\n",
        "            pooled = self.avgpool(feature)\n",
        "            pooled = pooled.view(pooled.size(0), -1)\n",
        "            pooled_features.append(pooled)\n",
        "\n",
        "        return tuple(pooled_features)\n",
        "\n",
        "\n",
        "class SwinBasicFeatureExtractor(nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "\n",
        "        super(SwinBasicFeatureExtractor, self).__init__()\n",
        "\n",
        "        self.model = timm.create_model('swin_base_patch4_window7_224',\n",
        "                                      pretrained=pretrained,\n",
        "                                      features_only=True)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        all_features = self.model(x)\n",
        "\n",
        "        pooled_features = []\n",
        "        for feature in all_features:\n",
        "            feature = feature.permute(0, 3, 1, 2)\n",
        "            pooled = self.avgpool(feature)\n",
        "            pooled = pooled.view(pooled.size(0), -1)\n",
        "            pooled_features.append(pooled)\n",
        "\n",
        "        return tuple(pooled_features[1:])"
      ],
      "metadata": {
        "id": "Hr4oJ4dfApKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SwinBasicFeatureExtractor(pretrained=True)\n",
        "input = torch.randn(1, 3, 224, 224)\n",
        "output = model(input)\n",
        "\n",
        "for feature in output:\n",
        "    print(feature.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLmNp0N9zqJV",
        "outputId": "51c0be68-3c55-4e56-ac9d-998c88ed5b4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 256])\n",
            "torch.Size([1, 512])\n",
            "torch.Size([1, 1024])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "\n",
        "class SparseAutoencoder(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim=2048, hidden_dim=1024, latent_dim=768, dropout_rate=0.3):\n",
        "        super(SparseAutoencoder, self).__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout_rate),\n",
        "\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout_rate),\n",
        "\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout_rate),\n",
        "\n",
        "            nn.Linear(hidden_dim, latent_dim),\n",
        "            nn.LayerNorm(latent_dim),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout_rate),\n",
        "\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout_rate),\n",
        "\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout_rate),\n",
        "\n",
        "            nn.Linear(hidden_dim, input_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Initialize weights\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                # Kaiming initialization for layers followed by ReLU\n",
        "                init.kaiming_uniform_(m.weight, a=0, nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.LayerNorm):\n",
        "                # Initialize LayerNorm with weight=1 and bias=0\n",
        "                init.ones_(m.weight)\n",
        "                init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        x_recon = self.decoder(z)\n",
        "        return x_recon, z\n"
      ],
      "metadata": {
        "id": "HLNY65W8AskS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BaselineModel(nn.Module):\n",
        "    def __init__(self, feature_extractor, num_classes=31):\n",
        "\n",
        "        super().__init__()\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        self.classifier = nn.Linear(768, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, _, f4 = self.feature_extractor(x)\n",
        "        logits = self.classifier(f4)\n",
        "        return logits, f4\n"
      ],
      "metadata": {
        "id": "Xjm7SG84A22J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UnifiedModelMultiBlockSAE(nn.Module):\n",
        "    def __init__(self,\n",
        "                 sae_dim_block2=128,\n",
        "                 sae_dim_block3=256,\n",
        "                 sae_dim_block4=256,\n",
        "                 num_classes=31):\n",
        "        super().__init__()\n",
        "\n",
        "        self.feature_extractor = SwinBasicFeatureExtractor(pretrained=True)\n",
        "        self.sae4 = SparseAutoencoder(input_dim=1024, latent_dim=1024)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(1024, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, _, f4 = self.feature_extractor(x)\n",
        "\n",
        "        x_recon4, z4 = self.sae4(f4)\n",
        "\n",
        "        class_logits = self.classifier(z4)\n",
        "\n",
        "        return (class_logits,\n",
        "                (x_recon4, z4),\n",
        "                (f4))"
      ],
      "metadata": {
        "id": "U-dyfxWWA3SC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Losses"
      ],
      "metadata": {
        "id": "41ugZ06WBPER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def irm_penalty(logits, labels):\n",
        "\n",
        "    scale = torch.tensor(1.0, requires_grad=True, device=logits.device)\n",
        "    loss_erm = F.cross_entropy(scale * logits, labels)\n",
        "\n",
        "    grad = torch.autograd.grad(loss_erm, [scale], create_graph=True)[0]\n",
        "\n",
        "    penalty = torch.sum(grad**2)\n",
        "    var = 0.0\n",
        "\n",
        "    return loss_erm, penalty, var"
      ],
      "metadata": {
        "id": "1L9aXvYeBRLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_sae_loss(recons, pools, zs, lambda_sparse=[1.0, 1.0, 1.0], lambda_reconstruction=[1.0, 1.0, 1.0]):\n",
        "\n",
        "    x2_recon, x3_recon, x4_recon = recons\n",
        "    f2p, f3p, f4p = pools\n",
        "    z2, z3, z4 = zs\n",
        "\n",
        "    rec_loss2 = F.mse_loss(x2_recon, f2p)\n",
        "    rec_loss3 = F.mse_loss(x3_recon, f3p)\n",
        "    rec_loss4 = F.mse_loss(x4_recon, f4p)\n",
        "\n",
        "    lambda_r1, lambda_r2, lambda_r3 = lambda_reconstruction\n",
        "    recon_loss_total = (lambda_r1 * rec_loss2 + lambda_r2 * rec_loss3 + lambda_r3 * rec_loss4)\n",
        "\n",
        "    l1_z2 = torch.mean(torch.abs(z2))\n",
        "    l1_z3 = torch.mean(torch.abs(z3))\n",
        "    l1_z4 = torch.mean(torch.abs(z4))\n",
        "    lambda_s1, lambda_s2, lambda_s3 = lambda_sparse\n",
        "    l1_sparsity_total = (lambda_s1 * l1_z2 + lambda_s2 * l1_z3 + lambda_s3 * l1_z4)\n",
        "\n",
        "    return recon_loss_total, l1_sparsity_total\n"
      ],
      "metadata": {
        "id": "5o_c5NnbBbB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trian and Evaluate Functions"
      ],
      "metadata": {
        "id": "HSt6tjAxBkrq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "9asqo-VoExhV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_main_irm_multi_sae_office(model,\n",
        "                              loader_source,\n",
        "                              test_loader,\n",
        "                              num_epochs=20,\n",
        "                              lr=1e-4,\n",
        "                              lr_sae=1e-4,\n",
        "                              lambda_irm=1.0,\n",
        "                              lambda_sae_rec=1.0,\n",
        "                              lambda_sae_sparse=1e-4,\n",
        "                              lambda_sparse=[1.0, 1.0, 1.0],\n",
        "                              lambda_reconstruction=[1.0, 1.0, 1.0],\n",
        "                              lambda_irm_pair=[1.0, 1.0, 1.0],\n",
        "                              device='cuda',\n",
        "                              verbose=False):\n",
        "\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    def sae_forward_splits(f2p, f3p, f4p):\n",
        "        x_recon2, z2 = model.sae2(f2p)\n",
        "        x_recon3, z3 = model.sae3(f3p)\n",
        "        x_recon4, z4 = model.sae4(f4p)\n",
        "        return x_recon2, z2, x_recon3, z3, x_recon4, z4\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        source_iter = iter(loader_source)\n",
        "        steps_per_epoch = len(source_iter)\n",
        "\n",
        "        for step in range(steps_per_epoch):\n",
        "\n",
        "            try:\n",
        "                x_s, y_s = next(source_iter)\n",
        "            except StopIteration:\n",
        "                source_iter = iter(loader_source)\n",
        "                x_s, y_s = next(source_iter)\n",
        "\n",
        "            x_s, y_s = x_s.to(device), y_s.to(device)\n",
        "\n",
        "            class_logits_s, _, _, _, _ = model(x_s)\n",
        "\n",
        "            loss_erm_s, penalty_s, var_s = irm_penalty(class_logits_s, y_s)\n",
        "\n",
        "            irm_loss = 0.5 * (loss_erm_s)\n",
        "            irm_pen  = 0.5 * (penalty_s)\n",
        "\n",
        "\n",
        "            w1, w2, w3 = lambda_irm_pair\n",
        "            loss_irm = w1 * (irm_loss) + w2 * (lambda_irm * irm_pen) + w3 * var_s\n",
        "\n",
        "\n",
        "            with torch.no_grad():\n",
        "                _, (x_recon2_s, z2_s), (x_recon3_s, z3_s), (x_recon4_s, z4_s), (f2p_s, f3p_s, f4p_s) = model(x_s)\n",
        "\n",
        "            x_recon2_s, z2_s, x_recon3_s, z3_s, x_recon4_s, z4_s = sae_forward_splits(f2p_s, f3p_s, f4p_s)\n",
        "\n",
        "            lambda_s1, lambda_s2, lambda_s3 = lambda_sparse\n",
        "            lambda_r1, lambda_r2, lambda_r3 = lambda_reconstruction\n",
        "\n",
        "            rec_loss2_s = F.mse_loss(x_recon2_s, f2p_s)\n",
        "            rec_loss2   = lambda_r1 * (rec_loss2_s)\n",
        "\n",
        "            rec_loss3_s = F.mse_loss(x_recon3_s, f3p_s)\n",
        "            rec_loss3   = lambda_r2 * (rec_loss3_s)\n",
        "\n",
        "            rec_loss4_s = F.mse_loss(x_recon4_s, f4p_s)\n",
        "            rec_loss4   = lambda_r3 * (rec_loss4_s)\n",
        "\n",
        "            rec_loss_total = (rec_loss2 + rec_loss3 + rec_loss4)\n",
        "\n",
        "            l1_2_s = torch.mean(torch.abs(z2_s))\n",
        "            l1_2   = lambda_s1 * (l1_2_s)\n",
        "\n",
        "            l1_3_s = torch.mean(torch.abs(z3_s))\n",
        "            l1_3   = lambda_s2 * (l1_3_s)\n",
        "\n",
        "            l1_4_s = torch.mean(torch.abs(z4_s))\n",
        "            l1_4   = lambda_s3 * (l1_4_s)\n",
        "\n",
        "            l1_sparsity = l1_2 + l1_3 + l1_4\n",
        "\n",
        "            sae_loss = lambda_sae_rec * rec_loss_total + lambda_sae_sparse * l1_sparsity\n",
        "\n",
        "            loss = loss_irm + sae_loss\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if (step+1) % 40 == 0 and verbose:\n",
        "                print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{step+1}/{steps_per_epoch}], \"\n",
        "                      f\"IRM Loss: {loss_irm.item():.4f}, SAE Loss: {sae_loss.item():.4f}\")\n",
        "\n",
        "        test_acc = evaluate(model, test_loader, device=device)\n",
        "        print(f\"** End of Epoch {epoch+1}/{num_epochs} | Test Accuracy: {test_acc:.2f}% **\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "9n1I3VJsEgb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_irm_sae_with_warmup_office(\n",
        "    batch_size=16,\n",
        "    num_warmup_epochs=5,\n",
        "    num_main_epochs=20,\n",
        "    lr=1e-4,\n",
        "    lr_sae=1e-4,\n",
        "    lambda_irm=1.0,\n",
        "    lambda_sae_rec=1.0,\n",
        "    lambda_sae_sparse=1e-4,\n",
        "    device='cuda',\n",
        "    loader=['A', 'W'],\n",
        "    model=None,\n",
        "    verbose=False,\n",
        "    lambda_sparse=[1.0, 1.0, 1.0],\n",
        "    lambda_reconstruction=[1.0, 1.0, 1.0],\n",
        "    lambda_irm_pair=[1.0, 1.0, 0.0],\n",
        "):\n",
        "\n",
        "\n",
        "    loader_amazon, loader_webcam, loader_dslr = get_office_data_loaders(batch_size)\n",
        "    loader_amazon_test, loader_webcam_test, loader_dslr_test = get_office_data_loaders(batch_size)\n",
        "\n",
        "    source, target = loader if loader is not None else ['A', 'W']\n",
        "\n",
        "    if source == 'A':\n",
        "        loader_source = loader_amazon\n",
        "    elif source == 'W':\n",
        "        loader_source = loader_webcam\n",
        "    elif source == 'D':\n",
        "        loader_source = loader_dslr\n",
        "\n",
        "    if target == 'A':\n",
        "        loader_target = loader_amazon_test\n",
        "    elif target == 'W':\n",
        "        loader_target = loader_webcam_test\n",
        "    elif target == 'D':\n",
        "        loader_target = loader_dslr_test\n",
        "\n",
        "\n",
        "    if model is None:\n",
        "        model = UnifiedModelMultiBlockSAE(512, 1024, 2048, 31)\n",
        "\n",
        "    print(\"===== Main Phase (IRM + SAE) =====\")\n",
        "\n",
        "    model = train_main_irm_multi_sae_office(model,\n",
        "                                            loader_source=loader_source,\n",
        "                                            test_loader=loader_target,\n",
        "                                            num_epochs=num_main_epochs,\n",
        "                                            lr=lr,\n",
        "                                            lr_sae=lr_sae,\n",
        "                                            lambda_irm=lambda_irm,\n",
        "                                            lambda_sae_rec=lambda_sae_rec,\n",
        "                                            lambda_sae_sparse=lambda_sae_sparse,\n",
        "                                            device=device,\n",
        "                                            verbose=verbose,\n",
        "                                            lambda_sparse=lambda_sparse,\n",
        "                                            lambda_reconstruction=lambda_reconstruction,\n",
        "                                            lambda_irm_pair=lambda_irm_pair\n",
        "                                            )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "fK8JO5c-Epys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Separate optm"
      ],
      "metadata": {
        "id": "VXlLEUc9IJC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_main_irm_multi_sae_office(\n",
        "    model,\n",
        "    loader_source,\n",
        "    test_loader,\n",
        "    num_epochs=20,\n",
        "    lr=1e-4,\n",
        "    lr_sae=1e-4,\n",
        "    lambda_irm=1.0,\n",
        "    lambda_sae_rec=1.0,\n",
        "    lambda_sae_sparse=1e-4,\n",
        "    lambda_sparse=[1.0, 1.0, 1.0],\n",
        "    lambda_reconstruction=[1.0, 1.0, 1.0],\n",
        "    lambda_irm_pair=[1.0, 1.0, 1.0],\n",
        "    device='cuda',\n",
        "    verbose=False\n",
        "):\n",
        "    import torch.optim as optim\n",
        "\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    def sae_forward_splits(f4p):\n",
        "        # x_recon2, z2 = model.sae2(f2p)\n",
        "        # x_recon3, z3 = model.sae3(f3p)\n",
        "        x_recon4, z4 = model.sae4(f4p)\n",
        "        return x_recon4, z4\n",
        "\n",
        "    # Define separate optimizers\n",
        "    # Optimizer for the feature extractor and classifier\n",
        "    params_rest = [\n",
        "        p for n, p in model.named_parameters()\n",
        "        if not (n.startswith('sae2') or n.startswith('sae3') or n.startswith('sae4'))\n",
        "    ]\n",
        "    # optimizer_rest = optim.SGD(params_rest, lr=lr, momentum=0.9)\n",
        "    optimizer_rest = optim.Adam(params_rest, lr=lr)\n",
        "    # Optimizer for the Sparse Autoencoders (sae2, sae3, sae4)\n",
        "    params_sae = list(model.sae4.parameters())\n",
        "    optimizer_sae = optim.Adam(params_sae, lr=lr_sae)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        source_iter = iter(loader_source)\n",
        "        steps_per_epoch = len(source_iter)\n",
        "\n",
        "        for step in range(steps_per_epoch):\n",
        "\n",
        "            try:\n",
        "                x_s, y_s = next(source_iter)\n",
        "            except StopIteration:\n",
        "                source_iter = iter(loader_source)\n",
        "                x_s, y_s = next(source_iter)\n",
        "\n",
        "            x_s, y_s = x_s.to(device), y_s.to(device)\n",
        "\n",
        "            # Forward pass for classification\n",
        "            class_logits_s, _, _ = model(x_s)\n",
        "\n",
        "            # Compute IRM loss\n",
        "            loss_erm_s, penalty_s, var_s = irm_penalty(class_logits_s, y_s)\n",
        "\n",
        "            irm_loss = 0.5 * loss_erm_s\n",
        "            irm_pen  = 0.5 * penalty_s\n",
        "\n",
        "            w1, w2, w3 = lambda_irm_pair\n",
        "            loss_irm = w1 * irm_loss + w2 * (lambda_irm * irm_pen) + w3 * var_s\n",
        "\n",
        "            # Forward pass for SAEs without tracking gradients\n",
        "            with torch.no_grad():\n",
        "                class_logits_s, (x_recon4_s, z4_s), (f4p_s) = model(x_s)\n",
        "\n",
        "            # Forward pass through SAEs to get reconstructions and latent vectors\n",
        "            x_recon4_s, z4_s = sae_forward_splits(f4p_s)\n",
        "\n",
        "            # Compute Reconstruction Loss\n",
        "            lambda_s1, lambda_s2, lambda_s3 = lambda_sparse\n",
        "            lambda_r1, lambda_r2, lambda_r3 = lambda_reconstruction\n",
        "\n",
        "\n",
        "            rec_loss4_s = F.mse_loss(x_recon4_s, f4p_s)\n",
        "            rec_loss4   = lambda_r3 * rec_loss4_s\n",
        "\n",
        "            rec_loss_total = rec_loss4\n",
        "\n",
        "            l1_4_s = torch.mean(torch.abs(z4_s))\n",
        "            l1_4   = lambda_s3 * l1_4_s\n",
        "\n",
        "            l1_sparsity = l1_4\n",
        "\n",
        "            # Total SAE Loss\n",
        "            sae_loss = lambda_sae_rec * rec_loss_total + lambda_sae_sparse * l1_sparsity\n",
        "\n",
        "            loss = loss_irm + sae_loss\n",
        "\n",
        "\n",
        "            # Zero gradients for both optimizers\n",
        "            optimizer_rest.zero_grad()\n",
        "            optimizer_sae.zero_grad()\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Update parameters\n",
        "            optimizer_rest.step()\n",
        "            optimizer_sae.step()\n",
        "\n",
        "            if (step+1) % 40 == 0 and verbose:\n",
        "                print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{step+1}/{steps_per_epoch}], \"\n",
        "                      f\"IRM Loss: {loss_irm.item():.4f}, SAE Loss: {sae_loss.item():.4f}\")\n",
        "\n",
        "        # Evaluation after each epoch\n",
        "        test_acc = evaluate(model, test_loader, device=device)\n",
        "        print(f\"** End of Epoch {epoch+1}/{num_epochs} | Test Accuracy: {test_acc:.2f}% **\")\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "rYRTCpINIVwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate"
      ],
      "metadata": {
        "id": "k9Y3ne2WE0z1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_baseline(model, loader, device='cuda'):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            logits, _ = model(x)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += y.size(0)\n",
        "    acc = 100.0 * correct / total\n",
        "    model.train()\n",
        "    return acc\n"
      ],
      "metadata": {
        "id": "I1JnoZwcEqPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, loader, device='cuda'):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            logits, _, _ = model(x)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += y.size(0)\n",
        "    acc = 100.0 * correct / total\n",
        "    return acc"
      ],
      "metadata": {
        "id": "9ZVa9RfWEs12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MAIN"
      ],
      "metadata": {
        "id": "5Efg0fiMHjya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ours"
      ],
      "metadata": {
        "id": "wyB1rTLlItrg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##A -> D **DO** **NOT** **RUN**"
      ],
      "metadata": {
        "id": "vrWNwJeBMfvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combination = [\"A\", \"D\"]\n",
        "print(f\"------------Combination: {combination}--------------\")\n",
        "lambda_irm_pair = [10.0, 4.0, 0.0]\n",
        "lambda_sparse=[0.1, 0.1, 0.1]\n",
        "\n",
        "trained_model_sae_office = train_model_irm_sae_with_warmup_office(\n",
        "            # model=trained_model_sae_office,\n",
        "            batch_size=32,\n",
        "            num_warmup_epochs=4,\n",
        "            num_main_epochs=10,\n",
        "            lr=1e-5,\n",
        "            lr_sae=5e-5,\n",
        "            lambda_irm=1.0,\n",
        "            lambda_sae_rec=2.0,\n",
        "            lambda_sae_sparse=2e-4,\n",
        "            device='cuda',\n",
        "            verbose=True,\n",
        "            lambda_irm_pair=lambda_irm_pair,\n",
        "            lambda_sparse=lambda_sparse,\n",
        "            loader=combination\n",
        "        )\n",
        "\n",
        "print(\"--------------------------------------------\")\n"
      ],
      "metadata": {
        "id": "ht88PjKfIuxE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71ecb9f9-6331-44db-da44-9d10bc128065"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------Combination: ['A', 'D']--------------\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Resuming download from 0 bytes (79538472 bytes left)...\n",
            "Resuming download from https://www.kaggle.com/api/v1/datasets/download/xixuhu/office31?dataset_version_number=1 (0/79538472) bytes left.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 75.9M/75.9M [00:04<00:00, 17.8MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "===== Main Phase (IRM + SAE) =====\n",
            "Epoch [1/10], Step [40/89], IRM Loss: 16.9484, SAE Loss: 24.5316\n",
            "Epoch [1/10], Step [80/89], IRM Loss: 15.9215, SAE Loss: 7.4968\n",
            "** End of Epoch 1/10 | Test Accuracy: 32.33% **\n",
            "Epoch [2/10], Step [40/89], IRM Loss: 10.5335, SAE Loss: 2.8149\n",
            "Epoch [2/10], Step [80/89], IRM Loss: 6.2005, SAE Loss: 3.8766\n",
            "** End of Epoch 2/10 | Test Accuracy: 76.10% **\n",
            "Epoch [3/10], Step [40/89], IRM Loss: 2.9101, SAE Loss: 3.6661\n",
            "Epoch [3/10], Step [80/89], IRM Loss: 2.2937, SAE Loss: 3.8711\n",
            "** End of Epoch 3/10 | Test Accuracy: 88.68% **\n",
            "Epoch [4/10], Step [40/89], IRM Loss: 1.3317, SAE Loss: 4.4993\n",
            "Epoch [4/10], Step [80/89], IRM Loss: 0.5690, SAE Loss: 3.6851\n",
            "** End of Epoch 4/10 | Test Accuracy: 91.57% **\n",
            "Epoch [5/10], Step [40/89], IRM Loss: 0.6646, SAE Loss: 3.2237\n",
            "Epoch [5/10], Step [80/89], IRM Loss: 0.2320, SAE Loss: 3.8896\n",
            "** End of Epoch 5/10 | Test Accuracy: 89.69% **\n",
            "Epoch [6/10], Step [40/89], IRM Loss: 0.4437, SAE Loss: 3.5886\n",
            "Epoch [6/10], Step [80/89], IRM Loss: 0.1690, SAE Loss: 3.5982\n",
            "** End of Epoch 6/10 | Test Accuracy: 91.45% **\n",
            "Epoch [7/10], Step [40/89], IRM Loss: 0.2137, SAE Loss: 3.5408\n",
            "Epoch [7/10], Step [80/89], IRM Loss: 0.2672, SAE Loss: 3.3501\n",
            "** End of Epoch 7/10 | Test Accuracy: 90.94% **\n",
            "Epoch [8/10], Step [40/89], IRM Loss: 0.2570, SAE Loss: 2.7499\n",
            "Epoch [8/10], Step [80/89], IRM Loss: 0.0821, SAE Loss: 3.3892\n",
            "** End of Epoch 8/10 | Test Accuracy: 91.70% **\n",
            "Epoch [9/10], Step [40/89], IRM Loss: 0.1135, SAE Loss: 3.4177\n",
            "Epoch [9/10], Step [80/89], IRM Loss: 0.0793, SAE Loss: 3.3864\n",
            "** End of Epoch 9/10 | Test Accuracy: 91.57% **\n",
            "Epoch [10/10], Step [40/89], IRM Loss: 0.0530, SAE Loss: 3.3021\n",
            "Epoch [10/10], Step [80/89], IRM Loss: 0.0824, SAE Loss: 2.5361\n",
            "** End of Epoch 10/10 | Test Accuracy: 91.45% **\n",
            "--------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##A -> W **DO** **NOT** **RUN**"
      ],
      "metadata": {
        "id": "zHDAoBjnMDFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lambda_irm_pair = [10.0, 4.0, 0.0]\n",
        "lambda_sparse=[0.1, 0.1, 0.1]\n",
        "\n",
        "trained_model_sae_office = train_model_irm_sae_with_warmup_office(\n",
        "            # model=trained_model_sae_office,\n",
        "            batch_size=32,\n",
        "            num_warmup_epochs=4,\n",
        "            num_main_epochs=10,\n",
        "            lr=1e-5,\n",
        "            lr_sae=5e-5,\n",
        "            lambda_irm=1.0,\n",
        "            lambda_sae_rec=2.0,\n",
        "            lambda_sae_sparse=2e-4,\n",
        "            device='cuda',\n",
        "            verbose=True,\n",
        "            lambda_irm_pair=lambda_irm_pair,\n",
        "            lambda_sparse=lambda_sparse\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vid9GyzYXnJX",
        "outputId": "562cab9a-52d2-40c8-ccfd-783a62ecd533"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "===== Main Phase (IRM + SAE) =====\n",
            "Epoch [1/10], Step [40/89], IRM Loss: 17.4886, SAE Loss: 36.3434\n",
            "Epoch [1/10], Step [80/89], IRM Loss: 16.9284, SAE Loss: 16.5139\n",
            "** End of Epoch 1/10 | Test Accuracy: 16.27% **\n",
            "Epoch [2/10], Step [40/89], IRM Loss: 11.7183, SAE Loss: 3.8549\n",
            "Epoch [2/10], Step [80/89], IRM Loss: 6.7545, SAE Loss: 4.2968\n",
            "** End of Epoch 2/10 | Test Accuracy: 78.31% **\n",
            "Epoch [3/10], Step [40/89], IRM Loss: 2.1402, SAE Loss: 4.0041\n",
            "Epoch [3/10], Step [80/89], IRM Loss: 2.5182, SAE Loss: 4.7085\n",
            "** End of Epoch 3/10 | Test Accuracy: 87.95% **\n",
            "Epoch [4/10], Step [40/89], IRM Loss: 1.2146, SAE Loss: 4.8893\n",
            "Epoch [4/10], Step [80/89], IRM Loss: 2.4019, SAE Loss: 4.2020\n",
            "** End of Epoch 4/10 | Test Accuracy: 89.36% **\n",
            "Epoch [5/10], Step [40/89], IRM Loss: 0.5383, SAE Loss: 4.2130\n",
            "Epoch [5/10], Step [80/89], IRM Loss: 0.8254, SAE Loss: 4.2853\n",
            "** End of Epoch 5/10 | Test Accuracy: 91.97% **\n",
            "Epoch [6/10], Step [40/89], IRM Loss: 0.2088, SAE Loss: 4.9362\n",
            "Epoch [6/10], Step [80/89], IRM Loss: 0.2334, SAE Loss: 4.9267\n",
            "** End of Epoch 6/10 | Test Accuracy: 91.97% **\n",
            "Epoch [7/10], Step [40/89], IRM Loss: 0.1490, SAE Loss: 4.6180\n",
            "Epoch [7/10], Step [80/89], IRM Loss: 0.3141, SAE Loss: 5.2176\n",
            "** End of Epoch 7/10 | Test Accuracy: 91.57% **\n",
            "Epoch [8/10], Step [40/89], IRM Loss: 0.1140, SAE Loss: 4.2205\n",
            "Epoch [8/10], Step [80/89], IRM Loss: 0.1024, SAE Loss: 4.2127\n",
            "** End of Epoch 8/10 | Test Accuracy: 92.37% **\n",
            "Epoch [9/10], Step [40/89], IRM Loss: 0.0712, SAE Loss: 4.7058\n",
            "Epoch [9/10], Step [80/89], IRM Loss: 0.1467, SAE Loss: 4.7731\n",
            "** End of Epoch 9/10 | Test Accuracy: 92.17% **\n",
            "Epoch [10/10], Step [40/89], IRM Loss: 0.0793, SAE Loss: 4.3806\n",
            "Epoch [10/10], Step [80/89], IRM Loss: 0.0563, SAE Loss: 4.1882\n",
            "** End of Epoch 10/10 | Test Accuracy: 92.97% **\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##W -> A **DO** **NOT** **RUN**"
      ],
      "metadata": {
        "id": "hi3LQBlkMlXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combination = [\"W\", \"A\"]\n",
        "print(f\"------------Combination: {combination}--------------\")\n",
        "lambda_irm_pair = [10.0, 4.0, 0.0]\n",
        "lambda_sparse=[0.1, 0.1, 0.1]\n",
        "\n",
        "trained_model_sae_office = train_model_irm_sae_with_warmup_office(\n",
        "            # model=trained_model_sae_office,\n",
        "            batch_size=32,\n",
        "            num_warmup_epochs=4,\n",
        "            num_main_epochs=10,\n",
        "            lr=1e-5,\n",
        "            lr_sae=5e-5,\n",
        "            lambda_irm=1.0,\n",
        "            lambda_sae_rec=2.0,\n",
        "            lambda_sae_sparse=2e-4,\n",
        "            device='cuda',\n",
        "            verbose=True,\n",
        "            lambda_irm_pair=lambda_irm_pair,\n",
        "            lambda_sparse=lambda_sparse,\n",
        "            loader=combination\n",
        "        )\n",
        "\n",
        "print(\"--------------------------------------------\")\n"
      ],
      "metadata": {
        "id": "Q2D7BGS-JocU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85fef4f2-5f44-418c-c68c-52eef1efb831"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------Combination: ['W', 'A']--------------\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "===== Main Phase (IRM + SAE) =====\n",
            "** End of Epoch 1/10 | Test Accuracy: 3.55% **\n",
            "** End of Epoch 2/10 | Test Accuracy: 3.80% **\n",
            "** End of Epoch 3/10 | Test Accuracy: 17.78% **\n",
            "** End of Epoch 4/10 | Test Accuracy: 12.64% **\n",
            "** End of Epoch 5/10 | Test Accuracy: 11.61% **\n",
            "** End of Epoch 6/10 | Test Accuracy: 18.14% **\n",
            "** End of Epoch 7/10 | Test Accuracy: 39.58% **\n",
            "** End of Epoch 8/10 | Test Accuracy: 62.62% **\n",
            "** End of Epoch 9/10 | Test Accuracy: 74.19% **\n",
            "** End of Epoch 10/10 | Test Accuracy: 76.93% **\n",
            "--------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## W -> D **DO** **NOT** **RUN**"
      ],
      "metadata": {
        "id": "nYRNGI5MMwmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combination = [\"W\", \"D\"]\n",
        "print(f\"------------Combination: {combination}--------------\")\n",
        "lambda_irm_pair = [10.0, 4.0, 0.0]\n",
        "lambda_sparse=[0.1, 0.1, 0.1]\n",
        "\n",
        "trained_model_sae_office = train_model_irm_sae_with_warmup_office(\n",
        "            # model=trained_model_sae_office,\n",
        "            batch_size=32,\n",
        "            num_warmup_epochs=4,\n",
        "            num_main_epochs=10,\n",
        "            lr=1e-5,\n",
        "            lr_sae=5e-5,\n",
        "            lambda_irm=1.0,\n",
        "            lambda_sae_rec=2.0,\n",
        "            lambda_sae_sparse=2e-4,\n",
        "            device='cuda',\n",
        "            verbose=True,\n",
        "            lambda_irm_pair=lambda_irm_pair,\n",
        "            lambda_sparse=lambda_sparse,\n",
        "            loader=combination\n",
        "        )\n",
        "\n",
        "print(\"--------------------------------------------\")\n"
      ],
      "metadata": {
        "id": "ptu2Pf0hMStI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5074f7f1-c208-4dea-e3d1-dee194b66442"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------Combination: ['W', 'D']--------------\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "===== Main Phase (IRM + SAE) =====\n",
            "** End of Epoch 1/10 | Test Accuracy: 3.77% **\n",
            "** End of Epoch 2/10 | Test Accuracy: 5.79% **\n",
            "** End of Epoch 3/10 | Test Accuracy: 19.25% **\n",
            "** End of Epoch 4/10 | Test Accuracy: 21.13% **\n",
            "** End of Epoch 5/10 | Test Accuracy: 30.69% **\n",
            "** End of Epoch 6/10 | Test Accuracy: 47.30% **\n",
            "** End of Epoch 7/10 | Test Accuracy: 68.81% **\n",
            "** End of Epoch 8/10 | Test Accuracy: 89.81% **\n",
            "** End of Epoch 9/10 | Test Accuracy: 98.36% **\n",
            "** End of Epoch 10/10 | Test Accuracy: 98.99% **\n",
            "--------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## D -> W **DO** **NOT** **RUN**"
      ],
      "metadata": {
        "id": "DP1jYfNVQ_iN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combination = [\"D\", \"W\"]\n",
        "print(f\"------------Combination: {combination}--------------\")\n",
        "lambda_irm_pair = [10.0, 4.0, 0.0]\n",
        "lambda_sparse=[0.1, 0.1, 0.1]\n",
        "\n",
        "trained_model_sae_office = train_model_irm_sae_with_warmup_office(\n",
        "            # model=trained_model_sae_office,\n",
        "            batch_size=32,\n",
        "            num_warmup_epochs=4,\n",
        "            num_main_epochs=10,\n",
        "            lr=1e-5,\n",
        "            lr_sae=5e-5,\n",
        "            lambda_irm=1.0,\n",
        "            lambda_sae_rec=2.0,\n",
        "            lambda_sae_sparse=2e-4,\n",
        "            device='cuda',\n",
        "            verbose=True,\n",
        "            lambda_irm_pair=lambda_irm_pair,\n",
        "            lambda_sparse=lambda_sparse,\n",
        "            loader=combination\n",
        "        )\n",
        "\n",
        "print(\"--------------------------------------------\")\n"
      ],
      "metadata": {
        "id": "E8Y4FC9dQ_P-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc716e70-b61f-4188-c334-06618c77f682"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------Combination: ['D', 'W']--------------\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "===== Main Phase (IRM + SAE) =====\n",
            "** End of Epoch 1/10 | Test Accuracy: 4.62% **\n",
            "** End of Epoch 2/10 | Test Accuracy: 22.49% **\n",
            "** End of Epoch 3/10 | Test Accuracy: 32.13% **\n",
            "** End of Epoch 4/10 | Test Accuracy: 42.77% **\n",
            "** End of Epoch 5/10 | Test Accuracy: 81.93% **\n",
            "** End of Epoch 6/10 | Test Accuracy: 98.19% **\n",
            "** End of Epoch 7/10 | Test Accuracy: 99.00% **\n",
            "** End of Epoch 8/10 | Test Accuracy: 99.40% **\n",
            "** End of Epoch 9/10 | Test Accuracy: 99.60% **\n",
            "** End of Epoch 10/10 | Test Accuracy: 99.60% **\n",
            "--------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## D -> A **DO** **NOT** **RUN**"
      ],
      "metadata": {
        "id": "viwb-kgBRJ_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combination = [\"D\", \"A\"]\n",
        "print(f\"------------Combination: {combination}--------------\")\n",
        "lambda_irm_pair = [10.0, 4.0, 0.0]\n",
        "lambda_sparse=[0.1, 0.1, 0.1]\n",
        "\n",
        "trained_model_sae_office = train_model_irm_sae_with_warmup_office(\n",
        "            # model=trained_model_sae_office,\n",
        "            batch_size=32,\n",
        "            num_warmup_epochs=4,\n",
        "            num_main_epochs=10,\n",
        "            lr=1e-5,\n",
        "            lr_sae=5e-5,\n",
        "            lambda_irm=1.0,\n",
        "            lambda_sae_rec=2.0,\n",
        "            lambda_sae_sparse=2e-4,\n",
        "            device='cuda',\n",
        "            verbose=True,\n",
        "            lambda_irm_pair=lambda_irm_pair,\n",
        "            lambda_sparse=lambda_sparse,\n",
        "            loader=combination\n",
        "        )\n",
        "\n",
        "print(\"--------------------------------------------\")\n"
      ],
      "metadata": {
        "id": "wzx4ihVERK8D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fb384a6-a61c-442d-ac5d-ee72f4af80d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------Combination: ['D', 'A']--------------\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "===== Main Phase (IRM + SAE) =====\n",
            "** End of Epoch 2/10 | Test Accuracy: 9.16% **\n",
            "** End of Epoch 3/10 | Test Accuracy: 7.24% **\n",
            "** End of Epoch 4/10 | Test Accuracy: 19.38% **\n",
            "** End of Epoch 5/10 | Test Accuracy: 42.28% **\n",
            "** End of Epoch 6/10 | Test Accuracy: 69.76% **\n",
            "** End of Epoch 7/10 | Test Accuracy: 76.11% **\n",
            "** End of Epoch 8/10 | Test Accuracy: 77.17% **\n",
            "** End of Epoch 9/10 | Test Accuracy: 77.64% **\n",
            "** End of Epoch 10/10 | Test Accuracy: 77.64% **\n",
            "--------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Base Vit accuracies"
      ],
      "metadata": {
        "id": "hFNlKjLoEcbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import swin_b\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def get_base_accuracy(loader=None):\n",
        "\n",
        "    # print(f\"------------Combination: {loader}--------------\")\n",
        "\n",
        "    loader_amazon, loader_webcam, loader_dslr = get_office_data_loaders(32)\n",
        "\n",
        "    source, target = loader if loader is not None else ['A', 'W']\n",
        "\n",
        "    print(source)\n",
        "    print(target)\n",
        "\n",
        "    if source == 'A':\n",
        "        loader_source = loader_amazon\n",
        "    elif source == 'W':\n",
        "        loader_source = loader_webcam\n",
        "    elif source == 'D':\n",
        "        loader_source = loader_dslr\n",
        "\n",
        "    if target == 'A':\n",
        "        loader_target = loader_amazon\n",
        "    elif target == 'W':\n",
        "        loader_target = loader_webcam\n",
        "    elif target == 'D':\n",
        "        loader_target = loader_dslr\n",
        "\n",
        "\n",
        "    model = swin_b(weights='IMAGENET1K_V1')\n",
        "    model.head = nn.Linear(1024, 31)\n",
        "    model = model.to(\"cuda\")\n",
        "\n",
        "    # optm = torch.optim.SGD(model.parameters(), lr=0.001, nesterov=True)\n",
        "    optm = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
        "    epochs = 10\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for x, y in (loader_source):\n",
        "            x, y = x.to(\"cuda\"), y.to(\"cuda\")\n",
        "\n",
        "            optm.zero_grad()\n",
        "            logits = model(x)\n",
        "            loss = F.cross_entropy(logits, y)\n",
        "            loss.backward()\n",
        "            optm.step()\n",
        "\n",
        "        print(f\"Epoch: {epoch+1}/{epochs} | Loss: {loss.item()}\")\n",
        "\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for x, y in (loader_target):\n",
        "                x, y = x.to(\"cuda\"), y.to(\"cuda\")\n",
        "\n",
        "                logits = model(x)\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "                correct += (preds == y).sum().item()\n",
        "                total += y.size(0)\n",
        "        acc = 100.0 * correct / total\n",
        "        print(f\"Epoch: {epoch+1}/{epochs} | Test Accuracy: {acc}\")\n",
        "\n",
        "        # print(\"--------------------------------------------\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YgJ7rEvZq0Ea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combinations = [[\"A\", \"W\"], [\"A\", \"D\"], [\"W\", \"A\"], [\"W\", \"D\"], [\"D\", \"W\"], [\"D\", \"A\"]]\n",
        "\n",
        "for combination in combinations:\n",
        "    print(f\"------------Combination: {combination}--------------\")\n",
        "    get_base_accuracy(loader=combination)\n",
        "    print(\"--------------------------------------------\")"
      ],
      "metadata": {
        "id": "8Gq9SGgzt-R6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80198a8c-185d-4654-d08f-d17621e42ba0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------Combination: ['A', 'W']--------------\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "A\n",
            "W\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/swin_b-68c6b09e.pth\" to /root/.cache/torch/hub/checkpoints/swin_b-68c6b09e.pth\n",
            "100%|██████████| 335M/335M [00:01<00:00, 221MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10 | Loss: 0.9589638113975525\n",
            "Epoch: 1/10 | Test Accuracy: 82.73092369477912\n",
            "Epoch: 2/10 | Loss: 0.07044864445924759\n",
            "Epoch: 2/10 | Test Accuracy: 88.35341365461848\n",
            "Epoch: 3/10 | Loss: 0.2872573137283325\n",
            "Epoch: 3/10 | Test Accuracy: 87.95180722891567\n",
            "Epoch: 4/10 | Loss: 0.021923918277025223\n",
            "Epoch: 4/10 | Test Accuracy: 88.35341365461848\n",
            "Epoch: 5/10 | Loss: 0.01937207765877247\n",
            "Epoch: 5/10 | Test Accuracy: 85.7429718875502\n",
            "Epoch: 6/10 | Loss: 0.00635898532345891\n",
            "Epoch: 6/10 | Test Accuracy: 88.15261044176707\n",
            "Epoch: 7/10 | Loss: 0.13682954013347626\n",
            "Epoch: 7/10 | Test Accuracy: 86.54618473895583\n",
            "Epoch: 8/10 | Loss: 0.6562557816505432\n",
            "Epoch: 8/10 | Test Accuracy: 84.13654618473896\n",
            "Epoch: 9/10 | Loss: 0.0012750837486237288\n",
            "Epoch: 9/10 | Test Accuracy: 88.15261044176707\n",
            "Epoch: 10/10 | Loss: 0.0003666205739136785\n",
            "Epoch: 10/10 | Test Accuracy: 88.15261044176707\n",
            "--------------------------------------------\n",
            "------------Combination: ['A', 'D']--------------\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "A\n",
            "D\n",
            "Epoch: 1/10 | Loss: 0.017085740342736244\n",
            "Epoch: 1/10 | Test Accuracy: 81.38364779874213\n",
            "Epoch: 2/10 | Loss: 0.00400099391117692\n",
            "Epoch: 2/10 | Test Accuracy: 83.64779874213836\n",
            "Epoch: 3/10 | Loss: 0.00114292127545923\n",
            "Epoch: 3/10 | Test Accuracy: 87.79874213836477\n",
            "Epoch: 4/10 | Loss: 0.06294664740562439\n",
            "Epoch: 4/10 | Test Accuracy: 84.52830188679245\n",
            "Epoch: 5/10 | Loss: 0.01475375983864069\n",
            "Epoch: 5/10 | Test Accuracy: 88.93081761006289\n",
            "Epoch: 6/10 | Loss: 0.18940392136573792\n",
            "Epoch: 6/10 | Test Accuracy: 88.0503144654088\n",
            "Epoch: 7/10 | Loss: 0.00019298121333122253\n",
            "Epoch: 7/10 | Test Accuracy: 86.91823899371069\n",
            "Epoch: 8/10 | Loss: 0.0009971652179956436\n",
            "Epoch: 8/10 | Test Accuracy: 84.27672955974843\n",
            "Epoch: 9/10 | Loss: 0.00022539935889653862\n",
            "Epoch: 9/10 | Test Accuracy: 85.28301886792453\n",
            "Epoch: 10/10 | Loss: 0.0005485698929987848\n",
            "Epoch: 10/10 | Test Accuracy: 85.9119496855346\n",
            "--------------------------------------------\n",
            "------------Combination: ['W', 'A']--------------\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "W\n",
            "A\n",
            "Epoch: 1/10 | Loss: 2.7751572132110596\n",
            "Epoch: 1/10 | Test Accuracy: 20.624778132765353\n",
            "Epoch: 2/10 | Loss: 1.6054151058197021\n",
            "Epoch: 2/10 | Test Accuracy: 44.72843450479233\n",
            "Epoch: 3/10 | Loss: 0.5215309262275696\n",
            "Epoch: 3/10 | Test Accuracy: 62.05182818601349\n",
            "Epoch: 4/10 | Loss: 0.21484148502349854\n",
            "Epoch: 4/10 | Test Accuracy: 67.83812566560171\n",
            "Epoch: 5/10 | Loss: 0.15450209379196167\n",
            "Epoch: 5/10 | Test Accuracy: 69.613063542776\n",
            "Epoch: 6/10 | Loss: 0.03122461773455143\n",
            "Epoch: 6/10 | Test Accuracy: 70.21654242101526\n",
            "Epoch: 7/10 | Loss: 0.05838733911514282\n",
            "Epoch: 7/10 | Test Accuracy: 70.50053248136315\n",
            "Epoch: 8/10 | Loss: 0.03817105293273926\n",
            "Epoch: 8/10 | Test Accuracy: 70.6070287539936\n",
            "Epoch: 9/10 | Loss: 0.00935797207057476\n",
            "Epoch: 9/10 | Test Accuracy: 70.25204117855876\n",
            "Epoch: 10/10 | Loss: 0.033108901232481\n",
            "Epoch: 10/10 | Test Accuracy: 70.50053248136315\n",
            "--------------------------------------------\n",
            "------------Combination: ['W', 'D']--------------\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "W\n",
            "D\n",
            "Epoch: 1/10 | Loss: 2.6740291118621826\n",
            "Epoch: 1/10 | Test Accuracy: 62.0125786163522\n",
            "Epoch: 2/10 | Loss: 1.2152117490768433\n",
            "Epoch: 2/10 | Test Accuracy: 85.78616352201257\n",
            "Epoch: 3/10 | Loss: 0.49857181310653687\n",
            "Epoch: 3/10 | Test Accuracy: 95.9748427672956\n",
            "Epoch: 4/10 | Loss: 0.1381465345621109\n",
            "Epoch: 4/10 | Test Accuracy: 97.61006289308176\n",
            "Epoch: 5/10 | Loss: 0.0528210885822773\n",
            "Epoch: 5/10 | Test Accuracy: 98.86792452830188\n",
            "Epoch: 6/10 | Loss: 0.08166345208883286\n",
            "Epoch: 6/10 | Test Accuracy: 98.74213836477988\n",
            "Epoch: 7/10 | Loss: 0.08004976809024811\n",
            "Epoch: 7/10 | Test Accuracy: 99.11949685534591\n",
            "Epoch: 8/10 | Loss: 0.020467903465032578\n",
            "Epoch: 8/10 | Test Accuracy: 98.99371069182389\n",
            "Epoch: 9/10 | Loss: 0.016761669889092445\n",
            "Epoch: 9/10 | Test Accuracy: 98.99371069182389\n",
            "Epoch: 10/10 | Loss: 0.016312964260578156\n",
            "Epoch: 10/10 | Test Accuracy: 98.86792452830188\n",
            "--------------------------------------------\n",
            "------------Combination: ['D', 'W']--------------\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "D\n",
            "W\n",
            "Epoch: 1/10 | Loss: 2.135963201522827\n",
            "Epoch: 1/10 | Test Accuracy: 70.28112449799197\n",
            "Epoch: 2/10 | Loss: 0.6259547472000122\n",
            "Epoch: 2/10 | Test Accuracy: 95.18072289156626\n",
            "Epoch: 3/10 | Loss: 0.1911499947309494\n",
            "Epoch: 3/10 | Test Accuracy: 98.99598393574297\n",
            "Epoch: 4/10 | Loss: 0.10070160031318665\n",
            "Epoch: 4/10 | Test Accuracy: 99.79919678714859\n",
            "Epoch: 5/10 | Loss: 0.032391034066677094\n",
            "Epoch: 5/10 | Test Accuracy: 100.0\n",
            "Epoch: 6/10 | Loss: 0.027272162958979607\n",
            "Epoch: 6/10 | Test Accuracy: 100.0\n",
            "Epoch: 7/10 | Loss: 0.020202448591589928\n",
            "Epoch: 7/10 | Test Accuracy: 99.79919678714859\n",
            "Epoch: 8/10 | Loss: 0.017105575650930405\n",
            "Epoch: 8/10 | Test Accuracy: 100.0\n",
            "Epoch: 9/10 | Loss: 0.012249201536178589\n",
            "Epoch: 9/10 | Test Accuracy: 99.79919678714859\n",
            "Epoch: 10/10 | Loss: 0.008499352261424065\n",
            "Epoch: 10/10 | Test Accuracy: 99.79919678714859\n",
            "--------------------------------------------\n",
            "------------Combination: ['D', 'A']--------------\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "D\n",
            "A\n",
            "Epoch: 1/10 | Loss: 2.2701187133789062\n",
            "Epoch: 1/10 | Test Accuracy: 34.54029108981186\n",
            "Epoch: 2/10 | Loss: 0.723124623298645\n",
            "Epoch: 2/10 | Test Accuracy: 62.47781327653532\n",
            "Epoch: 3/10 | Loss: 0.19809983670711517\n",
            "Epoch: 3/10 | Test Accuracy: 68.1221157259496\n",
            "Epoch: 4/10 | Loss: 0.07408776134252548\n",
            "Epoch: 4/10 | Test Accuracy: 69.96805111821087\n",
            "Epoch: 5/10 | Loss: 0.03323013335466385\n",
            "Epoch: 5/10 | Test Accuracy: 69.613063542776\n",
            "Epoch: 6/10 | Loss: 0.024143202230334282\n",
            "Epoch: 6/10 | Test Accuracy: 68.8675896343628\n",
            "Epoch: 7/10 | Loss: 0.013661043718457222\n",
            "Epoch: 7/10 | Test Accuracy: 69.32907348242811\n",
            "Epoch: 8/10 | Loss: 0.01271344069391489\n",
            "Epoch: 8/10 | Test Accuracy: 68.93858714944977\n",
            "Epoch: 9/10 | Loss: 0.026919087395071983\n",
            "Epoch: 9/10 | Test Accuracy: 69.57756478523251\n",
            "Epoch: 10/10 | Loss: 0.015539993532001972\n",
            "Epoch: 10/10 | Test Accuracy: 70.74902378416755\n",
            "--------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}