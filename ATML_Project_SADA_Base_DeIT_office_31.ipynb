{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "collapsed_sections": [
        "wp0Ewd14AnBj",
        "41ugZ06WBPER",
        "9asqo-VoExhV",
        "VXlLEUc9IJC6",
        "k9Y3ne2WE0z1",
        "vrWNwJeBMfvA",
        "zHDAoBjnMDFa",
        "hi3LQBlkMlXi",
        "nYRNGI5MMwmb",
        "DP1jYfNVQ_iN",
        "viwb-kgBRJ_I"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip installed install datasets"
      ],
      "metadata": {
        "id": "Z75mnFTp_ztC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8caf6e9-be5f-4440-d424-d6c50b06ae74",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIbI4UQ3_rn-"
      },
      "outputs": [],
      "source": [
        "# --------------------- Imports ------------------------- #\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torchvision import datasets, transforms, models\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.models import vit_b_16\n",
        "\n",
        "# Datasets\n",
        "# from datasets import load_dataset\n",
        "import kagglehub\n",
        "\n",
        "# Extra\n",
        "import os\n",
        "from copy import deepcopy\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import timm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Office-31"
      ],
      "metadata": {
        "id": "spAb68AuAgn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Office31(domain, transform):\n",
        "    # Download and set up the dataset path\n",
        "    path = kagglehub.dataset_download(\"xixuhu/office31\")\n",
        "    path = os.path.join(path, \"Office-31\", domain)\n",
        "    return datasets.ImageFolder(root=path, transform=transform)\n",
        "\n",
        "def get_office_data_loaders(batch_size):\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "    ])\n",
        "\n",
        "    amazon_data = Office31(\"amazon\", transform)\n",
        "    dslr_data = Office31(\"dslr\", transform)\n",
        "    webcam_data = Office31(\"webcam\", transform)\n",
        "\n",
        "    # Create data loaders\n",
        "    loader_amazon = DataLoader(amazon_data, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    loader_dslr = DataLoader(dslr_data, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    loader_webcam = DataLoader(webcam_data, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "\n",
        "    return loader_amazon, loader_dslr, loader_webcam\n"
      ],
      "metadata": {
        "id": "NnwP93vyAIgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pacs"
      ],
      "metadata": {
        "id": "QXWb4KN2AlBd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-KbFM2fmAmor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Py4n_wFMBSYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Cards"
      ],
      "metadata": {
        "id": "wp0Ewd14AnBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNetFeatureExtractor(nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        super().__init__()\n",
        "        base_model = torchvision.models.resnet50(pretrained=pretrained)\n",
        "        self.conv1 = base_model.conv1\n",
        "        self.bn1 = base_model.bn1\n",
        "        self.relu = base_model.relu\n",
        "        self.maxpool = base_model.maxpool\n",
        "        self.layer1 = base_model.layer1\n",
        "        self.layer2 = base_model.layer2\n",
        "        self.layer3 = base_model.layer3\n",
        "        self.layer4 = base_model.layer4\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        f_block1 = self.layer1(x)\n",
        "        f_block2 = self.layer2(f_block1)\n",
        "        f_block3 = self.layer3(f_block2)\n",
        "        f_block4 = self.layer4(f_block3)\n",
        "\n",
        "        return f_block2, f_block3, f_block4\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class VitBasicFeatureExtractor(nn.Module):\n",
        "\n",
        "    def __init__(self, pretrained=True, layers=[4, 8, 12]):\n",
        "        super().__init__()\n",
        "\n",
        "        # self.model = timm.create_model('vit_base_patch16_224.sam_in1k', pretrained=pretrained, features_only=True)\n",
        "        self.model = timm.create_model('vit_base_patch16_224', pretrained=pretrained, features_only=True)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        features = self.model(x)\n",
        "        pooled_features = []\n",
        "        for feature in features:\n",
        "            pooled = self.avgpool(feature)\n",
        "            pooled = pooled.view(pooled.size(0), -1)\n",
        "            pooled_features.append(pooled)\n",
        "\n",
        "        return tuple(pooled_features)\n",
        "\n",
        "\n",
        "class SwinBasicFeatureExtractor(nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "\n",
        "        super(SwinBasicFeatureExtractor, self).__init__()\n",
        "\n",
        "        self.model = timm.create_model('swin_base_patch4_window7_224',\n",
        "                                      pretrained=pretrained,\n",
        "                                      features_only=True)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        all_features = self.model(x)\n",
        "\n",
        "        pooled_features = []\n",
        "        for feature in all_features:\n",
        "            feature = feature.permute(0, 3, 1, 2)\n",
        "            pooled = self.avgpool(feature)\n",
        "            pooled = pooled.view(pooled.size(0), -1)\n",
        "            pooled_features.append(pooled)\n",
        "\n",
        "        return tuple(pooled_features[1:])\n",
        "\n",
        "class DeitBasicFeatureExtractor(nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = timm.create_model('deit_base_patch16_224.fb_in1k',\n",
        "                                      pretrained=pretrained,\n",
        "                                      features_only=True)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        all_features = self.model(x)\n",
        "\n",
        "        # return tuple(all_features)\n",
        "\n",
        "        pooled_features = []\n",
        "        for feature in all_features:\n",
        "            pooled = self.avgpool(feature)\n",
        "            pooled = pooled.view(pooled.size(0), -1)\n",
        "            pooled_features.append(pooled)\n",
        "\n",
        "        return tuple(pooled_features)"
      ],
      "metadata": {
        "id": "Hr4oJ4dfApKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "\n",
        "class SparseAutoencoder(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim=2048, hidden_dim=1024, latent_dim=768, dropout_rate=0.3):\n",
        "        super(SparseAutoencoder, self).__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout_rate),\n",
        "\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout_rate),\n",
        "\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout_rate),\n",
        "\n",
        "            nn.Linear(hidden_dim, latent_dim),\n",
        "            nn.LayerNorm(latent_dim),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout_rate),\n",
        "\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout_rate),\n",
        "\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout_rate),\n",
        "\n",
        "            nn.Linear(hidden_dim, input_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Initialize weights\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                # Kaiming initialization for layers followed by ReLU\n",
        "                init.kaiming_uniform_(m.weight, a=0, nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.LayerNorm):\n",
        "                # Initialize LayerNorm with weight=1 and bias=0\n",
        "                init.ones_(m.weight)\n",
        "                init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        x_recon = self.decoder(z)\n",
        "        return x_recon, z\n"
      ],
      "metadata": {
        "id": "HLNY65W8AskS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BaselineModel(nn.Module):\n",
        "    def __init__(self, feature_extractor, num_classes=31):\n",
        "\n",
        "        super().__init__()\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        self.classifier = nn.Linear(768, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, _, f4 = self.feature_extractor(x)\n",
        "        logits = self.classifier(f4)\n",
        "        return logits, f4\n"
      ],
      "metadata": {
        "id": "Xjm7SG84A22J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UnifiedModelMultiBlockSAE(nn.Module):\n",
        "    def __init__(self,\n",
        "                 sae_dim_block2=128,\n",
        "                 sae_dim_block3=256,\n",
        "                 sae_dim_block4=256,\n",
        "                 num_classes=31):\n",
        "        super().__init__()\n",
        "\n",
        "        self.feature_extractor = DeitBasicFeatureExtractor(pretrained=True)\n",
        "        self.sae4 = SparseAutoencoder(input_dim=768, latent_dim=768)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(768, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, _, f4 = self.feature_extractor(x)\n",
        "\n",
        "        x_recon4, z4 = self.sae4(f4)\n",
        "\n",
        "        class_logits = self.classifier(z4)\n",
        "\n",
        "        return (class_logits,\n",
        "                (x_recon4, z4),\n",
        "                (f4))"
      ],
      "metadata": {
        "id": "U-dyfxWWA3SC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Losses"
      ],
      "metadata": {
        "id": "41ugZ06WBPER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def irm_penalty(logits, labels):\n",
        "\n",
        "    scale = torch.tensor(1.0, requires_grad=True, device=logits.device)\n",
        "    loss_erm = F.cross_entropy(scale * logits, labels)\n",
        "\n",
        "    grad = torch.autograd.grad(loss_erm, [scale], create_graph=True)[0]\n",
        "\n",
        "    penalty = torch.sum(grad**2)\n",
        "    var = 0.0\n",
        "\n",
        "    return loss_erm, penalty, var"
      ],
      "metadata": {
        "id": "1L9aXvYeBRLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_sae_loss(recons, pools, zs, lambda_sparse=[1.0, 1.0, 1.0], lambda_reconstruction=[1.0, 1.0, 1.0]):\n",
        "\n",
        "    x2_recon, x3_recon, x4_recon = recons\n",
        "    f2p, f3p, f4p = pools\n",
        "    z2, z3, z4 = zs\n",
        "\n",
        "    rec_loss2 = F.mse_loss(x2_recon, f2p)\n",
        "    rec_loss3 = F.mse_loss(x3_recon, f3p)\n",
        "    rec_loss4 = F.mse_loss(x4_recon, f4p)\n",
        "\n",
        "    lambda_r1, lambda_r2, lambda_r3 = lambda_reconstruction\n",
        "    recon_loss_total = (lambda_r1 * rec_loss2 + lambda_r2 * rec_loss3 + lambda_r3 * rec_loss4)\n",
        "\n",
        "    l1_z2 = torch.mean(torch.abs(z2))\n",
        "    l1_z3 = torch.mean(torch.abs(z3))\n",
        "    l1_z4 = torch.mean(torch.abs(z4))\n",
        "    lambda_s1, lambda_s2, lambda_s3 = lambda_sparse\n",
        "    l1_sparsity_total = (lambda_s1 * l1_z2 + lambda_s2 * l1_z3 + lambda_s3 * l1_z4)\n",
        "\n",
        "    return recon_loss_total, l1_sparsity_total\n"
      ],
      "metadata": {
        "id": "5o_c5NnbBbB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trian and Evaluate Functions"
      ],
      "metadata": {
        "id": "HSt6tjAxBkrq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "9asqo-VoExhV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_main_irm_multi_sae_office(model,\n",
        "                              loader_source,\n",
        "                              test_loader,\n",
        "                              num_epochs=20,\n",
        "                              lr=1e-4,\n",
        "                              lr_sae=1e-4,\n",
        "                              lambda_irm=1.0,\n",
        "                              lambda_sae_rec=1.0,\n",
        "                              lambda_sae_sparse=1e-4,\n",
        "                              lambda_sparse=[1.0, 1.0, 1.0],\n",
        "                              lambda_reconstruction=[1.0, 1.0, 1.0],\n",
        "                              lambda_irm_pair=[1.0, 1.0, 1.0],\n",
        "                              device='cuda',\n",
        "                              verbose=False):\n",
        "\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    def sae_forward_splits(f2p, f3p, f4p):\n",
        "        x_recon2, z2 = model.sae2(f2p)\n",
        "        x_recon3, z3 = model.sae3(f3p)\n",
        "        x_recon4, z4 = model.sae4(f4p)\n",
        "        return x_recon2, z2, x_recon3, z3, x_recon4, z4\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        source_iter = iter(loader_source)\n",
        "        steps_per_epoch = len(source_iter)\n",
        "\n",
        "        for step in range(steps_per_epoch):\n",
        "\n",
        "            try:\n",
        "                x_s, y_s = next(source_iter)\n",
        "            except StopIteration:\n",
        "                source_iter = iter(loader_source)\n",
        "                x_s, y_s = next(source_iter)\n",
        "\n",
        "            x_s, y_s = x_s.to(device), y_s.to(device)\n",
        "\n",
        "            class_logits_s, _, _, _, _ = model(x_s)\n",
        "\n",
        "            loss_erm_s, penalty_s, var_s = irm_penalty(class_logits_s, y_s)\n",
        "\n",
        "            irm_loss = 0.5 * (loss_erm_s)\n",
        "            irm_pen  = 0.5 * (penalty_s)\n",
        "\n",
        "\n",
        "            w1, w2, w3 = lambda_irm_pair\n",
        "            loss_irm = w1 * (irm_loss) + w2 * (lambda_irm * irm_pen) + w3 * var_s\n",
        "\n",
        "\n",
        "            with torch.no_grad():\n",
        "                _, (x_recon2_s, z2_s), (x_recon3_s, z3_s), (x_recon4_s, z4_s), (f2p_s, f3p_s, f4p_s) = model(x_s)\n",
        "\n",
        "            x_recon2_s, z2_s, x_recon3_s, z3_s, x_recon4_s, z4_s = sae_forward_splits(f2p_s, f3p_s, f4p_s)\n",
        "\n",
        "            lambda_s1, lambda_s2, lambda_s3 = lambda_sparse\n",
        "            lambda_r1, lambda_r2, lambda_r3 = lambda_reconstruction\n",
        "\n",
        "            rec_loss2_s = F.mse_loss(x_recon2_s, f2p_s)\n",
        "            rec_loss2   = lambda_r1 * (rec_loss2_s)\n",
        "\n",
        "            rec_loss3_s = F.mse_loss(x_recon3_s, f3p_s)\n",
        "            rec_loss3   = lambda_r2 * (rec_loss3_s)\n",
        "\n",
        "            rec_loss4_s = F.mse_loss(x_recon4_s, f4p_s)\n",
        "            rec_loss4   = lambda_r3 * (rec_loss4_s)\n",
        "\n",
        "            rec_loss_total = (rec_loss2 + rec_loss3 + rec_loss4)\n",
        "\n",
        "            l1_2_s = torch.mean(torch.abs(z2_s))\n",
        "            l1_2   = lambda_s1 * (l1_2_s)\n",
        "\n",
        "            l1_3_s = torch.mean(torch.abs(z3_s))\n",
        "            l1_3   = lambda_s2 * (l1_3_s)\n",
        "\n",
        "            l1_4_s = torch.mean(torch.abs(z4_s))\n",
        "            l1_4   = lambda_s3 * (l1_4_s)\n",
        "\n",
        "            l1_sparsity = l1_2 + l1_3 + l1_4\n",
        "\n",
        "            sae_loss = lambda_sae_rec * rec_loss_total + lambda_sae_sparse * l1_sparsity\n",
        "\n",
        "            loss = loss_irm + sae_loss\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if (step+1) % 40 == 0 and verbose:\n",
        "                print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{step+1}/{steps_per_epoch}], \"\n",
        "                      f\"IRM Loss: {loss_irm.item():.4f}, SAE Loss: {sae_loss.item():.4f}\")\n",
        "\n",
        "        test_acc = evaluate(model, test_loader, device=device)\n",
        "        print(f\"** End of Epoch {epoch+1}/{num_epochs} | Test Accuracy: {test_acc:.2f}% **\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "9n1I3VJsEgb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_irm_sae_with_warmup_office(\n",
        "    batch_size=16,\n",
        "    num_warmup_epochs=5,\n",
        "    num_main_epochs=20,\n",
        "    lr=1e-4,\n",
        "    lr_sae=1e-4,\n",
        "    lambda_irm=1.0,\n",
        "    lambda_sae_rec=1.0,\n",
        "    lambda_sae_sparse=1e-4,\n",
        "    device='cuda',\n",
        "    loader=['A', 'W'],\n",
        "    model=None,\n",
        "    verbose=False,\n",
        "    lambda_sparse=[1.0, 1.0, 1.0],\n",
        "    lambda_reconstruction=[1.0, 1.0, 1.0],\n",
        "    lambda_irm_pair=[1.0, 1.0, 0.0],\n",
        "):\n",
        "\n",
        "\n",
        "    loader_amazon, loader_webcam, loader_dslr = get_office_data_loaders(batch_size)\n",
        "    loader_amazon_test, loader_webcam_test, loader_dslr_test = get_office_data_loaders(batch_size)\n",
        "\n",
        "    source, target = loader if loader is not None else ['A', 'W']\n",
        "\n",
        "    if source == 'A':\n",
        "        loader_source = loader_amazon\n",
        "    elif source == 'W':\n",
        "        loader_source = loader_webcam\n",
        "    elif source == 'D':\n",
        "        loader_source = loader_dslr\n",
        "\n",
        "    if target == 'A':\n",
        "        loader_target = loader_amazon_test\n",
        "    elif target == 'W':\n",
        "        loader_target = loader_webcam_test\n",
        "    elif target == 'D':\n",
        "        loader_target = loader_dslr_test\n",
        "\n",
        "\n",
        "    if model is None:\n",
        "        model = UnifiedModelMultiBlockSAE(512, 1024, 2048, 31)\n",
        "\n",
        "    print(\"===== Main Phase (IRM + SAE) =====\")\n",
        "\n",
        "    model = train_main_irm_multi_sae_office(model,\n",
        "                                            loader_source=loader_source,\n",
        "                                            test_loader=loader_target,\n",
        "                                            num_epochs=num_main_epochs,\n",
        "                                            lr=lr,\n",
        "                                            lr_sae=lr_sae,\n",
        "                                            lambda_irm=lambda_irm,\n",
        "                                            lambda_sae_rec=lambda_sae_rec,\n",
        "                                            lambda_sae_sparse=lambda_sae_sparse,\n",
        "                                            device=device,\n",
        "                                            verbose=verbose,\n",
        "                                            lambda_sparse=lambda_sparse,\n",
        "                                            lambda_reconstruction=lambda_reconstruction,\n",
        "                                            lambda_irm_pair=lambda_irm_pair\n",
        "                                            )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "fK8JO5c-Epys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Separate optm"
      ],
      "metadata": {
        "id": "VXlLEUc9IJC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_main_irm_multi_sae_office(\n",
        "    model,\n",
        "    loader_source,\n",
        "    test_loader,\n",
        "    num_epochs=20,\n",
        "    lr=1e-4,\n",
        "    lr_sae=1e-4,\n",
        "    lambda_irm=1.0,\n",
        "    lambda_sae_rec=1.0,\n",
        "    lambda_sae_sparse=1e-4,\n",
        "    lambda_sparse=[1.0, 1.0, 1.0],\n",
        "    lambda_reconstruction=[1.0, 1.0, 1.0],\n",
        "    lambda_irm_pair=[1.0, 1.0, 1.0],\n",
        "    device='cuda',\n",
        "    verbose=False\n",
        "):\n",
        "    import torch.optim as optim\n",
        "\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    def sae_forward_splits(f4p):\n",
        "        # x_recon2, z2 = model.sae2(f2p)\n",
        "        # x_recon3, z3 = model.sae3(f3p)\n",
        "        x_recon4, z4 = model.sae4(f4p)\n",
        "        return x_recon4, z4\n",
        "\n",
        "    # Define separate optimizers\n",
        "    # Optimizer for the feature extractor and classifier\n",
        "    params_rest = [\n",
        "        p for n, p in model.named_parameters()\n",
        "        if not (n.startswith('sae2') or n.startswith('sae3') or n.startswith('sae4'))\n",
        "    ]\n",
        "    # optimizer_rest = optim.SGD(params_rest, lr=lr, momentum=0.9)\n",
        "    optimizer_rest = optim.Adam(params_rest, lr=lr)\n",
        "    # Optimizer for the Sparse Autoencoders (sae2, sae3, sae4)\n",
        "    params_sae = list(model.sae4.parameters())\n",
        "    optimizer_sae = optim.Adam(params_sae, lr=lr_sae)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        source_iter = iter(loader_source)\n",
        "        steps_per_epoch = len(source_iter)\n",
        "\n",
        "        for step in range(steps_per_epoch):\n",
        "\n",
        "            try:\n",
        "                x_s, y_s = next(source_iter)\n",
        "            except StopIteration:\n",
        "                source_iter = iter(loader_source)\n",
        "                x_s, y_s = next(source_iter)\n",
        "\n",
        "            x_s, y_s = x_s.to(device), y_s.to(device)\n",
        "\n",
        "            # Forward pass for classification\n",
        "            class_logits_s, _, _ = model(x_s)\n",
        "\n",
        "            # Compute IRM loss\n",
        "            loss_erm_s, penalty_s, var_s = irm_penalty(class_logits_s, y_s)\n",
        "\n",
        "            irm_loss = 0.5 * loss_erm_s\n",
        "            irm_pen  = 0.5 * penalty_s\n",
        "\n",
        "            w1, w2, w3 = lambda_irm_pair\n",
        "            loss_irm = w1 * irm_loss + w2 * (lambda_irm * irm_pen) + w3 * var_s\n",
        "\n",
        "            # Forward pass for SAEs without tracking gradients\n",
        "            with torch.no_grad():\n",
        "                class_logits_s, (x_recon4_s, z4_s), (f4p_s) = model(x_s)\n",
        "\n",
        "            # Forward pass through SAEs to get reconstructions and latent vectors\n",
        "            x_recon4_s, z4_s = sae_forward_splits(f4p_s)\n",
        "\n",
        "            # Compute Reconstruction Loss\n",
        "            lambda_s1, lambda_s2, lambda_s3 = lambda_sparse\n",
        "            lambda_r1, lambda_r2, lambda_r3 = lambda_reconstruction\n",
        "\n",
        "\n",
        "            rec_loss4_s = F.mse_loss(x_recon4_s, f4p_s)\n",
        "            rec_loss4   = lambda_r3 * rec_loss4_s\n",
        "\n",
        "            rec_loss_total = rec_loss4\n",
        "\n",
        "            l1_4_s = torch.mean(torch.abs(z4_s))\n",
        "            l1_4   = lambda_s3 * l1_4_s\n",
        "\n",
        "            l1_sparsity = l1_4\n",
        "\n",
        "            # Total SAE Loss\n",
        "            sae_loss = lambda_sae_rec * rec_loss_total + lambda_sae_sparse * l1_sparsity\n",
        "\n",
        "            loss = loss_irm + sae_loss\n",
        "\n",
        "\n",
        "            # Zero gradients for both optimizers\n",
        "            optimizer_rest.zero_grad()\n",
        "            optimizer_sae.zero_grad()\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Update parameters\n",
        "            optimizer_rest.step()\n",
        "            optimizer_sae.step()\n",
        "\n",
        "            if (step+1) % 40 == 0 and verbose:\n",
        "                print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{step+1}/{steps_per_epoch}], \"\n",
        "                      f\"IRM Loss: {loss_irm.item():.4f}, SAE Loss: {sae_loss.item():.4f}\")\n",
        "\n",
        "        # Evaluation after each epoch\n",
        "        test_acc = evaluate(model, test_loader, device=device)\n",
        "        print(f\"** End of Epoch {epoch+1}/{num_epochs} | Test Accuracy: {test_acc:.2f}% **\")\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "rYRTCpINIVwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate"
      ],
      "metadata": {
        "id": "k9Y3ne2WE0z1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_baseline(model, loader, device='cuda'):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            logits, _ = model(x)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += y.size(0)\n",
        "    acc = 100.0 * correct / total\n",
        "    model.train()\n",
        "    return acc\n"
      ],
      "metadata": {
        "id": "I1JnoZwcEqPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, loader, device='cuda'):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            logits, _, _ = model(x)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += y.size(0)\n",
        "    acc = 100.0 * correct / total\n",
        "    return acc"
      ],
      "metadata": {
        "id": "9ZVa9RfWEs12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MAIN"
      ],
      "metadata": {
        "id": "5Efg0fiMHjya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ours"
      ],
      "metadata": {
        "id": "wyB1rTLlItrg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##A -> D **DO** **NOT** **RUN**"
      ],
      "metadata": {
        "id": "vrWNwJeBMfvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combination = [\"A\", \"D\"]\n",
        "print(f\"------------Combination: {combination}--------------\")\n",
        "lambda_irm_pair = [10.0, 4.0, 0.0]\n",
        "lambda_sparse=[0.1, 0.1, 0.1]\n",
        "\n",
        "trained_model_sae_office = train_model_irm_sae_with_warmup_office(\n",
        "            # model=trained_model_sae_office,\n",
        "            batch_size=32,\n",
        "            num_warmup_epochs=4,\n",
        "            num_main_epochs=15,\n",
        "            lr=1e-6,\n",
        "            lr_sae=5e-5,\n",
        "            lambda_irm=1.0,\n",
        "            lambda_sae_rec=2.0,\n",
        "            lambda_sae_sparse=2e-4,\n",
        "            device='cuda',\n",
        "            verbose=True,\n",
        "            lambda_irm_pair=lambda_irm_pair,\n",
        "            lambda_sparse=lambda_sparse,\n",
        "            loader=combination\n",
        "        )\n",
        "\n",
        "print(\"--------------------------------------------\")\n"
      ],
      "metadata": {
        "id": "ht88PjKfIuxE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e9f712a-3463-4176-b925-a6157084d264"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------Combination: ['A', 'D']--------------\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "===== Main Phase (IRM + SAE) =====\n",
            "Epoch [1/15], Step [40/89], IRM Loss: 17.0498, SAE Loss: 27.8333\n",
            "Epoch [1/15], Step [80/89], IRM Loss: 15.9341, SAE Loss: 23.7531\n",
            "** End of Epoch 1/15 | Test Accuracy: 31.19% **\n",
            "Epoch [2/15], Step [40/89], IRM Loss: 13.0841, SAE Loss: 17.6362\n",
            "Epoch [2/15], Step [80/89], IRM Loss: 9.8573, SAE Loss: 20.0936\n",
            "** End of Epoch 2/15 | Test Accuracy: 63.14% **\n",
            "Epoch [3/15], Step [40/89], IRM Loss: 6.4672, SAE Loss: 19.9149\n",
            "Epoch [3/15], Step [80/89], IRM Loss: 6.2289, SAE Loss: 20.3605\n",
            "** End of Epoch 3/15 | Test Accuracy: 73.08% **\n",
            "Epoch [4/15], Step [40/89], IRM Loss: 4.9000, SAE Loss: 21.4384\n",
            "Epoch [4/15], Step [80/89], IRM Loss: 3.3247, SAE Loss: 20.0976\n",
            "** End of Epoch 4/15 | Test Accuracy: 76.23% **\n",
            "Epoch [5/15], Step [40/89], IRM Loss: 4.1440, SAE Loss: 19.6685\n",
            "Epoch [5/15], Step [80/89], IRM Loss: 2.3622, SAE Loss: 21.9492\n",
            "** End of Epoch 5/15 | Test Accuracy: 78.62% **\n",
            "Epoch [6/15], Step [40/89], IRM Loss: 3.3617, SAE Loss: 19.1321\n",
            "Epoch [6/15], Step [80/89], IRM Loss: 2.2272, SAE Loss: 20.4551\n",
            "** End of Epoch 6/15 | Test Accuracy: 77.23% **\n",
            "Epoch [7/15], Step [40/89], IRM Loss: 2.2978, SAE Loss: 18.6859\n",
            "Epoch [7/15], Step [80/89], IRM Loss: 1.3600, SAE Loss: 17.1286\n",
            "** End of Epoch 7/15 | Test Accuracy: 80.88% **\n",
            "Epoch [8/15], Step [40/89], IRM Loss: 0.7912, SAE Loss: 19.9275\n",
            "Epoch [8/15], Step [80/89], IRM Loss: 1.4315, SAE Loss: 19.3496\n",
            "** End of Epoch 8/15 | Test Accuracy: 79.37% **\n",
            "Epoch [9/15], Step [40/89], IRM Loss: 0.9956, SAE Loss: 17.9277\n",
            "Epoch [9/15], Step [80/89], IRM Loss: 1.5060, SAE Loss: 21.4726\n",
            "** End of Epoch 9/15 | Test Accuracy: 81.13% **\n",
            "Epoch [10/15], Step [40/89], IRM Loss: 1.2419, SAE Loss: 18.6306\n",
            "Epoch [10/15], Step [80/89], IRM Loss: 0.4012, SAE Loss: 19.2435\n",
            "** End of Epoch 10/15 | Test Accuracy: 80.88% **\n",
            "Epoch [11/15], Step [40/89], IRM Loss: 0.3832, SAE Loss: 18.6450\n",
            "Epoch [11/15], Step [80/89], IRM Loss: 0.9265, SAE Loss: 18.9062\n",
            "** End of Epoch 11/15 | Test Accuracy: 81.38% **\n",
            "Epoch [12/15], Step [40/89], IRM Loss: 1.0993, SAE Loss: 19.4145\n",
            "Epoch [12/15], Step [80/89], IRM Loss: 0.2794, SAE Loss: 19.1626\n",
            "** End of Epoch 12/15 | Test Accuracy: 81.01% **\n",
            "Epoch [13/15], Step [40/89], IRM Loss: 0.3523, SAE Loss: 19.4147\n",
            "Epoch [13/15], Step [80/89], IRM Loss: 0.7481, SAE Loss: 17.1608\n",
            "** End of Epoch 13/15 | Test Accuracy: 80.63% **\n",
            "Epoch [14/15], Step [40/89], IRM Loss: 0.2222, SAE Loss: 17.7462\n",
            "Epoch [14/15], Step [80/89], IRM Loss: 0.1802, SAE Loss: 19.4914\n",
            "** End of Epoch 14/15 | Test Accuracy: 80.75% **\n",
            "Epoch [15/15], Step [40/89], IRM Loss: 0.3341, SAE Loss: 18.2766\n",
            "Epoch [15/15], Step [80/89], IRM Loss: 0.3171, SAE Loss: 17.7792\n",
            "** End of Epoch 15/15 | Test Accuracy: 80.13% **\n",
            "--------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##A -> W **DO** **NOT** **RUN**"
      ],
      "metadata": {
        "id": "zHDAoBjnMDFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lambda_irm_pair = [10.0, 4.0, 0.0]\n",
        "lambda_sparse=[0.1, 0.1, 0.1]\n",
        "\n",
        "trained_model_sae_office = train_model_irm_sae_with_warmup_office(\n",
        "            # model=trained_model_sae_office,\n",
        "            batch_size=32,\n",
        "            num_warmup_epochs=4,\n",
        "            num_main_epochs=15,\n",
        "            lr=1e-6,\n",
        "            lr_sae=5e-5,\n",
        "            lambda_irm=1.0,\n",
        "            lambda_sae_rec=2.0,\n",
        "            lambda_sae_sparse=2e-4,\n",
        "            device='cuda',\n",
        "            verbose=True,\n",
        "            lambda_irm_pair=lambda_irm_pair,\n",
        "            lambda_sparse=lambda_sparse\n",
        "        )"
      ],
      "metadata": {
        "id": "vid9GyzYXnJX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed95bda3-a522-4ead-b4ad-3d22ab0c8c7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "===== Main Phase (IRM + SAE) =====\n",
            "Epoch [1/15], Step [40/89], IRM Loss: 17.3859, SAE Loss: 29.8607\n",
            "Epoch [1/15], Step [80/89], IRM Loss: 16.7197, SAE Loss: 24.8834\n",
            "** End of Epoch 1/15 | Test Accuracy: 38.76% **\n",
            "Epoch [2/15], Step [40/89], IRM Loss: 13.4108, SAE Loss: 17.3458\n",
            "Epoch [2/15], Step [80/89], IRM Loss: 9.6741, SAE Loss: 19.0737\n",
            "** End of Epoch 2/15 | Test Accuracy: 45.78% **\n",
            "Epoch [3/15], Step [40/89], IRM Loss: 6.9941, SAE Loss: 20.3981\n",
            "Epoch [3/15], Step [80/89], IRM Loss: 5.6337, SAE Loss: 20.8559\n",
            "** End of Epoch 3/15 | Test Accuracy: 66.87% **\n",
            "Epoch [4/15], Step [40/89], IRM Loss: 5.1578, SAE Loss: 20.9708\n",
            "Epoch [4/15], Step [80/89], IRM Loss: 4.4381, SAE Loss: 22.5657\n",
            "** End of Epoch 4/15 | Test Accuracy: 74.30% **\n",
            "Epoch [5/15], Step [40/89], IRM Loss: 4.0525, SAE Loss: 19.4942\n",
            "Epoch [5/15], Step [80/89], IRM Loss: 3.0195, SAE Loss: 22.0497\n",
            "** End of Epoch 5/15 | Test Accuracy: 80.12% **\n",
            "Epoch [6/15], Step [40/89], IRM Loss: 0.6559, SAE Loss: 23.1317\n",
            "Epoch [6/15], Step [80/89], IRM Loss: 1.3669, SAE Loss: 21.3870\n",
            "** End of Epoch 6/15 | Test Accuracy: 80.12% **\n",
            "Epoch [7/15], Step [40/89], IRM Loss: 1.0162, SAE Loss: 22.0558\n",
            "Epoch [7/15], Step [80/89], IRM Loss: 0.6219, SAE Loss: 22.5347\n",
            "** End of Epoch 7/15 | Test Accuracy: 79.32% **\n",
            "Epoch [8/15], Step [40/89], IRM Loss: 2.2284, SAE Loss: 20.1405\n",
            "Epoch [8/15], Step [80/89], IRM Loss: 1.4635, SAE Loss: 20.7611\n",
            "** End of Epoch 8/15 | Test Accuracy: 79.92% **\n",
            "Epoch [9/15], Step [40/89], IRM Loss: 0.9782, SAE Loss: 19.7760\n",
            "Epoch [9/15], Step [80/89], IRM Loss: 1.0214, SAE Loss: 22.5914\n",
            "** End of Epoch 9/15 | Test Accuracy: 79.92% **\n",
            "Epoch [10/15], Step [40/89], IRM Loss: 1.4097, SAE Loss: 22.0729\n",
            "Epoch [10/15], Step [80/89], IRM Loss: 0.7482, SAE Loss: 21.5073\n",
            "** End of Epoch 10/15 | Test Accuracy: 81.33% **\n",
            "Epoch [11/15], Step [40/89], IRM Loss: 0.7259, SAE Loss: 19.4956\n",
            "Epoch [11/15], Step [80/89], IRM Loss: 0.2603, SAE Loss: 20.6054\n",
            "** End of Epoch 11/15 | Test Accuracy: 80.72% **\n",
            "Epoch [12/15], Step [40/89], IRM Loss: 0.9710, SAE Loss: 19.9186\n",
            "Epoch [12/15], Step [80/89], IRM Loss: 0.4004, SAE Loss: 17.7312\n",
            "** End of Epoch 12/15 | Test Accuracy: 81.33% **\n",
            "Epoch [13/15], Step [40/89], IRM Loss: 0.2911, SAE Loss: 19.3322\n",
            "Epoch [13/15], Step [80/89], IRM Loss: 0.4943, SAE Loss: 19.6203\n",
            "** End of Epoch 13/15 | Test Accuracy: 80.72% **\n",
            "Epoch [14/15], Step [40/89], IRM Loss: 0.6436, SAE Loss: 18.5133\n",
            "Epoch [14/15], Step [80/89], IRM Loss: 0.2199, SAE Loss: 19.6696\n",
            "** End of Epoch 14/15 | Test Accuracy: 81.73% **\n",
            "Epoch [15/15], Step [40/89], IRM Loss: 0.2045, SAE Loss: 20.7456\n",
            "Epoch [15/15], Step [80/89], IRM Loss: 0.3393, SAE Loss: 18.2963\n",
            "** End of Epoch 15/15 | Test Accuracy: 81.33% **\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##W -> A **DO** **NOT** **RUN**"
      ],
      "metadata": {
        "id": "hi3LQBlkMlXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combination = [\"W\", \"A\"]\n",
        "print(f\"------------Combination: {combination}--------------\")\n",
        "lambda_irm_pair = [10.0, 4.0, 0.0]\n",
        "lambda_sparse=[0.1, 0.1, 0.1]\n",
        "\n",
        "trained_model_sae_office = train_model_irm_sae_with_warmup_office(\n",
        "            # model=trained_model_sae_office,\n",
        "            batch_size=32,\n",
        "            num_warmup_epochs=4,\n",
        "            num_main_epochs=15,\n",
        "            lr=1e-6,\n",
        "            lr_sae=5e-5,\n",
        "            lambda_irm=1.0,\n",
        "            lambda_sae_rec=2.0,\n",
        "            lambda_sae_sparse=2e-4,\n",
        "            device='cuda',\n",
        "            verbose=True,\n",
        "            lambda_irm_pair=lambda_irm_pair,\n",
        "            lambda_sparse=lambda_sparse,\n",
        "            loader=combination\n",
        "        )\n",
        "\n",
        "print(\"--------------------------------------------\")\n"
      ],
      "metadata": {
        "id": "Q2D7BGS-JocU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09377ac4-8d65-49a8-f9ba-70c57aba4024"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------Combination: ['W', 'A']--------------\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "===== Main Phase (IRM + SAE) =====\n",
            "** End of Epoch 1/15 | Test Accuracy: 6.64% **\n",
            "** End of Epoch 2/15 | Test Accuracy: 7.14% **\n",
            "** End of Epoch 3/15 | Test Accuracy: 9.76% **\n",
            "** End of Epoch 4/15 | Test Accuracy: 5.18% **\n",
            "** End of Epoch 5/15 | Test Accuracy: 8.56% **\n",
            "** End of Epoch 6/15 | Test Accuracy: 12.25% **\n",
            "** End of Epoch 8/15 | Test Accuracy: 17.61% **\n",
            "** End of Epoch 9/15 | Test Accuracy: 24.46% **\n",
            "** End of Epoch 10/15 | Test Accuracy: 30.99% **\n",
            "** End of Epoch 11/15 | Test Accuracy: 35.32% **\n",
            "** End of Epoch 12/15 | Test Accuracy: 42.60% **\n",
            "** End of Epoch 13/15 | Test Accuracy: 47.00% **\n",
            "** End of Epoch 14/15 | Test Accuracy: 53.85% **\n",
            "** End of Epoch 15/15 | Test Accuracy: 56.12% **\n",
            "--------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## W -> D **DO** **NOT** **RUN**"
      ],
      "metadata": {
        "id": "nYRNGI5MMwmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combination = [\"W\", \"D\"]\n",
        "print(f\"------------Combination: {combination}--------------\")\n",
        "lambda_irm_pair = [10.0, 4.0, 0.0]\n",
        "lambda_sparse=[0.1, 0.1, 0.1]\n",
        "\n",
        "trained_model_sae_office = train_model_irm_sae_with_warmup_office(\n",
        "            # model=trained_model_sae_office,\n",
        "            batch_size=32,\n",
        "            num_warmup_epochs=4,\n",
        "            num_main_epochs=15,\n",
        "            lr=1e-6,\n",
        "            lr_sae=5e-5,\n",
        "            lambda_irm=1.0,\n",
        "            lambda_sae_rec=2.0,\n",
        "            lambda_sae_sparse=2e-4,\n",
        "            device='cuda',\n",
        "            verbose=True,\n",
        "            lambda_irm_pair=lambda_irm_pair,\n",
        "            lambda_sparse=lambda_sparse,\n",
        "            loader=combination\n",
        "        )\n",
        "\n",
        "print(\"--------------------------------------------\")\n"
      ],
      "metadata": {
        "id": "ptu2Pf0hMStI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48ab7e79-509c-4ec4-cd78-621706681371"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------Combination: ['W', 'D']--------------\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "===== Main Phase (IRM + SAE) =====\n",
            "** End of Epoch 1/15 | Test Accuracy: 10.31% **\n",
            "** End of Epoch 2/15 | Test Accuracy: 18.36% **\n",
            "** End of Epoch 3/15 | Test Accuracy: 27.30% **\n",
            "** End of Epoch 4/15 | Test Accuracy: 12.96% **\n",
            "** End of Epoch 5/15 | Test Accuracy: 25.28% **\n",
            "** End of Epoch 6/15 | Test Accuracy: 37.36% **\n",
            "** End of Epoch 7/15 | Test Accuracy: 38.99% **\n",
            "** End of Epoch 8/15 | Test Accuracy: 53.21% **\n",
            "** End of Epoch 9/15 | Test Accuracy: 61.76% **\n",
            "** End of Epoch 10/15 | Test Accuracy: 69.94% **\n",
            "** End of Epoch 11/15 | Test Accuracy: 74.97% **\n",
            "** End of Epoch 12/15 | Test Accuracy: 84.91% **\n",
            "** End of Epoch 13/15 | Test Accuracy: 87.80% **\n",
            "** End of Epoch 14/15 | Test Accuracy: 89.69% **\n",
            "** End of Epoch 15/15 | Test Accuracy: 91.45% **\n",
            "--------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## D -> W **DO** **NOT** **RUN**"
      ],
      "metadata": {
        "id": "DP1jYfNVQ_iN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combination = [\"D\", \"W\"]\n",
        "print(f\"------------Combination: {combination}--------------\")\n",
        "lambda_irm_pair = [10.0, 4.0, 0.0]\n",
        "lambda_sparse=[0.1, 0.1, 0.1]\n",
        "\n",
        "trained_model_sae_office = train_model_irm_sae_with_warmup_office(\n",
        "            # model=trained_model_sae_office,\n",
        "            batch_size=32,\n",
        "            num_warmup_epochs=4,\n",
        "            num_main_epochs=15,\n",
        "            lr=1e-6,\n",
        "            lr_sae=5e-5,\n",
        "            lambda_irm=1.0,\n",
        "            lambda_sae_rec=2.0,\n",
        "            lambda_sae_sparse=2e-4,\n",
        "            device='cuda',\n",
        "            verbose=True,\n",
        "            lambda_irm_pair=lambda_irm_pair,\n",
        "            lambda_sparse=lambda_sparse,\n",
        "            loader=combination\n",
        "        )\n",
        "\n",
        "print(\"--------------------------------------------\")\n"
      ],
      "metadata": {
        "id": "E8Y4FC9dQ_P-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84762c24-76e6-41f6-f1c1-330ae72ca436"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------Combination: ['D', 'W']--------------\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "===== Main Phase (IRM + SAE) =====\n",
            "** End of Epoch 1/15 | Test Accuracy: 14.66% **\n",
            "** End of Epoch 2/15 | Test Accuracy: 31.33% **\n",
            "** End of Epoch 3/15 | Test Accuracy: 19.88% **\n",
            "** End of Epoch 4/15 | Test Accuracy: 33.94% **\n",
            "** End of Epoch 5/15 | Test Accuracy: 47.99% **\n",
            "** End of Epoch 6/15 | Test Accuracy: 60.24% **\n",
            "** End of Epoch 7/15 | Test Accuracy: 67.67% **\n",
            "** End of Epoch 8/15 | Test Accuracy: 77.51% **\n",
            "** End of Epoch 9/15 | Test Accuracy: 85.54% **\n",
            "** End of Epoch 10/15 | Test Accuracy: 93.78% **\n",
            "** End of Epoch 11/15 | Test Accuracy: 96.39% **\n",
            "** End of Epoch 12/15 | Test Accuracy: 97.59% **\n",
            "** End of Epoch 13/15 | Test Accuracy: 98.39% **\n",
            "** End of Epoch 14/15 | Test Accuracy: 98.59% **\n",
            "** End of Epoch 15/15 | Test Accuracy: 98.59% **\n",
            "--------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## D -> A **DO** **NOT** **RUN**"
      ],
      "metadata": {
        "id": "viwb-kgBRJ_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combination = [\"D\", \"A\"]\n",
        "print(f\"------------Combination: {combination}--------------\")\n",
        "lambda_irm_pair = [10.0, 4.0, 0.0]\n",
        "lambda_sparse=[0.1, 0.1, 0.1]\n",
        "\n",
        "trained_model_sae_office = train_model_irm_sae_with_warmup_office(\n",
        "            # model=trained_model_sae_office,\n",
        "            batch_size=32,\n",
        "            num_warmup_epochs=4,\n",
        "            num_main_epochs=15,\n",
        "            lr=1e-6,\n",
        "            lr_sae=5e-5,\n",
        "            lambda_irm=1.0,\n",
        "            lambda_sae_rec=2.0,\n",
        "            lambda_sae_sparse=2e-4,\n",
        "            device='cuda',\n",
        "            verbose=True,\n",
        "            lambda_irm_pair=lambda_irm_pair,\n",
        "            lambda_sparse=lambda_sparse,\n",
        "            loader=combination\n",
        "        )\n",
        "\n",
        "print(\"--------------------------------------------\")\n"
      ],
      "metadata": {
        "id": "wzx4ihVERK8D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dba0a32-d068-4392-a2ab-488b14744ec9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------Combination: ['D', 'A']--------------\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "===== Main Phase (IRM + SAE) =====\n",
            "** End of Epoch 1/15 | Test Accuracy: 8.31% **\n",
            "** End of Epoch 2/15 | Test Accuracy: 17.22% **\n",
            "** End of Epoch 3/15 | Test Accuracy: 20.87% **\n",
            "** End of Epoch 4/15 | Test Accuracy: 17.07% **\n",
            "** End of Epoch 5/15 | Test Accuracy: 19.17% **\n",
            "** End of Epoch 6/15 | Test Accuracy: 32.02% **\n",
            "** End of Epoch 7/15 | Test Accuracy: 43.84% **\n",
            "** End of Epoch 8/15 | Test Accuracy: 52.96% **\n",
            "** End of Epoch 9/15 | Test Accuracy: 59.96% **\n",
            "** End of Epoch 10/15 | Test Accuracy: 63.47% **\n",
            "** End of Epoch 11/15 | Test Accuracy: 66.42% **\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7d2200302950>Exception ignored in: \n",
            "<function _MultiProcessingDataLoaderIter.__del__ at 0x7d2200302950>Traceback (most recent call last):\n",
            "\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
            "    self._shutdown_workers()    \n",
            "self._shutdown_workers()  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
            "\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
            "    if w.is_alive():    \n",
            "if w.is_alive():  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "\n",
            "      File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'AssertionError\n",
            ": Exception ignored in: AssertionErrorcan only test a child process: <function _MultiProcessingDataLoaderIter.__del__ at 0x7d2200302950>\n",
            "\n",
            "can only test a child processTraceback (most recent call last):\n",
            "\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "Exception ignored in:   File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "<function _MultiProcessingDataLoaderIter.__del__ at 0x7d2200302950>    \n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'Traceback (most recent call last):\n",
            "\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
            "AssertionError    : self._shutdown_workers()can only test a child process\n",
            "\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7d2200302950>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "Exception ignored in:     <function _MultiProcessingDataLoaderIter.__del__ at 0x7d2200302950>assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "\n",
            "Traceback (most recent call last):\n",
            "AssertionError  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
            ": can only test a child process    \n",
            "self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7d2200302950>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
            "    self._shutdown_workers()Exception ignored in: \n",
            "<function _MultiProcessingDataLoaderIter.__del__ at 0x7d2200302950>  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
            "\n",
            "    Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
            "if w.is_alive():\n",
            "      File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "self._shutdown_workers()    \n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "    AssertionErrorif w.is_alive():: \n",
            "can only test a child process  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** End of Epoch 12/15 | Test Accuracy: 67.23% **\n",
            "** End of Epoch 13/15 | Test Accuracy: 68.23% **\n",
            "** End of Epoch 14/15 | Test Accuracy: 68.48% **\n",
            "** End of Epoch 15/15 | Test Accuracy: 68.83% **\n",
            "--------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Base Vit accuracies"
      ],
      "metadata": {
        "id": "hFNlKjLoEcbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import vit_b_16\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import DeiTForImageClassification\n",
        "\n",
        "\n",
        "def get_base_accuracy(loader=None):\n",
        "\n",
        "    # print(f\"------------Combination: {loader}--------------\")\n",
        "\n",
        "    loader_amazon, loader_webcam, loader_dslr = get_office_data_loaders(32)\n",
        "\n",
        "    source, target = loader if loader is not None else ['A', 'W']\n",
        "\n",
        "    if source == 'A':\n",
        "        loader_source = loader_amazon\n",
        "    elif source == 'W':\n",
        "        loader_source = loader_webcam\n",
        "    elif source == 'D':\n",
        "        loader_source = loader_dslr\n",
        "\n",
        "    if target == 'A':\n",
        "        loader_target = loader_amazon\n",
        "    elif target == 'W':\n",
        "        loader_target = loader_webcam\n",
        "    elif target == 'D':\n",
        "        loader_target = loader_dslr\n",
        "\n",
        "\n",
        "    model = DeiTForImageClassification.from_pretrained(\"facebook/deit-base-distilled-patch16-224\")\n",
        "    model.classifier = nn.Linear(768, 31)\n",
        "    model = model.to(\"cuda\")\n",
        "\n",
        "    optm = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
        "    epochs = 10\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for x, y in (loader_source):\n",
        "            x, y = x.to(\"cuda\"), y.to(\"cuda\")\n",
        "\n",
        "            optm.zero_grad()\n",
        "            logits = model(x).logits\n",
        "            loss = F.cross_entropy(logits, y)\n",
        "            loss.backward()\n",
        "            optm.step()\n",
        "\n",
        "        print(f\"Epoch: {epoch+1}/{epochs} | Loss: {loss.item()}\")\n",
        "\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for x, y in (loader_target):\n",
        "                x, y = x.to(\"cuda\"), y.to(\"cuda\")\n",
        "\n",
        "                logits = model(x).logits\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "                correct += (preds == y).sum().item()\n",
        "                total += y.size(0)\n",
        "        acc = 100.0 * correct / total\n",
        "        print(f\"Epoch: {epoch+1}/{epochs} | Test Accuracy: {acc}\")\n",
        "\n",
        "        # print(\"--------------------------------------------\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YgJ7rEvZq0Ea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combinations = [[\"A\", \"W\"], [\"A\", \"D\"], [\"W\", \"A\"], [\"W\", \"D\"], [\"D\", \"W\"], [\"D\", \"A\"]]\n",
        "\n",
        "for combination in combinations:\n",
        "    print(f\"------------Combination: {combination}--------------\")\n",
        "    get_base_accuracy(loader=combination)\n",
        "    print(\"--------------------------------------------\")"
      ],
      "metadata": {
        "id": "8Gq9SGgzt-R6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8809d0e8-a2d4-4b6a-fdf7-3f1ebfff4d41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------Combination: ['A', 'W']--------------\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DeiTForImageClassification were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10 | Loss: 0.03427192568778992\n",
            "Epoch: 1/10 | Test Accuracy: 77.30923694779116\n",
            "Epoch: 2/10 | Loss: 0.07789202779531479\n",
            "Epoch: 2/10 | Test Accuracy: 79.31726907630522\n",
            "Epoch: 3/10 | Loss: 0.052264418452978134\n",
            "Epoch: 3/10 | Test Accuracy: 80.12048192771084\n",
            "Epoch: 4/10 | Loss: 0.0008509114268235862\n",
            "Epoch: 4/10 | Test Accuracy: 79.51807228915662\n",
            "Epoch: 5/10 | Loss: 0.01712827943265438\n",
            "Epoch: 5/10 | Test Accuracy: 80.32128514056225\n",
            "Epoch: 6/10 | Loss: 0.0013896104646846652\n",
            "Epoch: 6/10 | Test Accuracy: 80.72289156626506\n",
            "Epoch: 7/10 | Loss: 0.0007952864980325103\n",
            "Epoch: 7/10 | Test Accuracy: 80.92369477911646\n",
            "Epoch: 8/10 | Loss: 0.00029702542815357447\n",
            "Epoch: 8/10 | Test Accuracy: 80.72289156626506\n",
            "Epoch: 9/10 | Loss: 0.0013797297142446041\n",
            "Epoch: 9/10 | Test Accuracy: 80.72289156626506\n",
            "Epoch: 10/10 | Loss: 0.007620549760758877\n",
            "Epoch: 10/10 | Test Accuracy: 80.32128514056225\n",
            "--------------------------------------------\n",
            "------------Combination: ['A', 'D']--------------\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DeiTForImageClassification were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10 | Loss: 0.06954265385866165\n",
            "Epoch: 1/10 | Test Accuracy: 74.71698113207547\n",
            "Epoch: 2/10 | Loss: 0.006148708052933216\n",
            "Epoch: 2/10 | Test Accuracy: 76.22641509433963\n",
            "Epoch: 3/10 | Loss: 0.002998382318764925\n",
            "Epoch: 3/10 | Test Accuracy: 78.74213836477988\n",
            "Epoch: 4/10 | Loss: 0.012148083187639713\n",
            "Epoch: 4/10 | Test Accuracy: 78.23899371069183\n",
            "Epoch: 5/10 | Loss: 0.009702547453343868\n",
            "Epoch: 5/10 | Test Accuracy: 78.11320754716981\n",
            "Epoch: 6/10 | Loss: 0.0006071869283914566\n",
            "Epoch: 6/10 | Test Accuracy: 78.49056603773585\n",
            "Epoch: 7/10 | Loss: 0.0017064546700567007\n",
            "Epoch: 7/10 | Test Accuracy: 78.74213836477988\n",
            "Epoch: 8/10 | Loss: 0.0002203936892328784\n",
            "Epoch: 8/10 | Test Accuracy: 78.61635220125787\n",
            "Epoch: 9/10 | Loss: 0.00015209948469419032\n",
            "Epoch: 9/10 | Test Accuracy: 78.74213836477988\n",
            "Epoch: 10/10 | Loss: 0.0007362039177678525\n",
            "Epoch: 10/10 | Test Accuracy: 78.49056603773585\n",
            "--------------------------------------------\n",
            "------------Combination: ['W', 'A']--------------\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DeiTForImageClassification were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10 | Loss: 1.7352815866470337\n",
            "Epoch: 1/10 | Test Accuracy: 26.90805821796237\n",
            "Epoch: 2/10 | Loss: 0.14262782037258148\n",
            "Epoch: 2/10 | Test Accuracy: 51.01171458998935\n",
            "Epoch: 3/10 | Loss: 0.020084500312805176\n",
            "Epoch: 3/10 | Test Accuracy: 55.307064252751154\n",
            "Epoch: 4/10 | Loss: 0.009953547269105911\n",
            "Epoch: 4/10 | Test Accuracy: 56.69151579694711\n",
            "Epoch: 5/10 | Loss: 0.007418349385261536\n",
            "Epoch: 5/10 | Test Accuracy: 56.86900958466454\n",
            "Epoch: 6/10 | Loss: 0.005166002083569765\n",
            "Epoch: 6/10 | Test Accuracy: 56.79801206957757\n",
            "Epoch: 7/10 | Loss: 0.0047587500885128975\n",
            "Epoch: 7/10 | Test Accuracy: 57.117500887468935\n",
            "Epoch: 8/10 | Loss: 0.004499915987253189\n",
            "Epoch: 8/10 | Test Accuracy: 57.011004614838484\n",
            "Epoch: 9/10 | Loss: 0.0035709217190742493\n",
            "Epoch: 9/10 | Test Accuracy: 57.08200212992545\n",
            "Epoch: 10/10 | Loss: 0.0036504885647445917\n",
            "Epoch: 10/10 | Test Accuracy: 57.18849840255591\n",
            "--------------------------------------------\n",
            "------------Combination: ['W', 'D']--------------\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DeiTForImageClassification were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10 | Loss: 1.192643165588379\n",
            "Epoch: 1/10 | Test Accuracy: 80.62893081761007\n",
            "Epoch: 2/10 | Loss: 0.0785270407795906\n",
            "Epoch: 2/10 | Test Accuracy: 97.23270440251572\n",
            "Epoch: 3/10 | Loss: 0.02375122159719467\n",
            "Epoch: 3/10 | Test Accuracy: 97.86163522012579\n",
            "Epoch: 4/10 | Loss: 0.00916258618235588\n",
            "Epoch: 4/10 | Test Accuracy: 98.49056603773585\n",
            "Epoch: 5/10 | Loss: 0.007910201326012611\n",
            "Epoch: 5/10 | Test Accuracy: 98.61635220125787\n",
            "Epoch: 6/10 | Loss: 0.005907063838094473\n",
            "Epoch: 6/10 | Test Accuracy: 98.61635220125787\n",
            "Epoch: 7/10 | Loss: 0.004419818986207247\n",
            "Epoch: 7/10 | Test Accuracy: 98.61635220125787\n",
            "Epoch: 8/10 | Loss: 0.005289257038384676\n",
            "Epoch: 8/10 | Test Accuracy: 98.61635220125787\n",
            "Epoch: 9/10 | Loss: 0.003232485381886363\n",
            "Epoch: 9/10 | Test Accuracy: 98.61635220125787\n",
            "Epoch: 10/10 | Loss: 0.0033467216417193413\n",
            "Epoch: 10/10 | Test Accuracy: 98.61635220125787\n",
            "--------------------------------------------\n",
            "------------Combination: ['D', 'W']--------------\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DeiTForImageClassification were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10 | Loss: 0.39330098032951355\n",
            "Epoch: 1/10 | Test Accuracy: 97.38955823293173\n",
            "Epoch: 2/10 | Loss: 0.09654019773006439\n",
            "Epoch: 2/10 | Test Accuracy: 99.59839357429719\n",
            "Epoch: 3/10 | Loss: 0.012457349337637424\n",
            "Epoch: 3/10 | Test Accuracy: 100.0\n",
            "Epoch: 4/10 | Loss: 0.00811581127345562\n",
            "Epoch: 4/10 | Test Accuracy: 100.0\n",
            "Epoch: 5/10 | Loss: 0.005905048921704292\n",
            "Epoch: 5/10 | Test Accuracy: 100.0\n",
            "Epoch: 6/10 | Loss: 0.004387938883155584\n",
            "Epoch: 6/10 | Test Accuracy: 100.0\n",
            "Epoch: 7/10 | Loss: 0.003571375273168087\n",
            "Epoch: 7/10 | Test Accuracy: 100.0\n",
            "Epoch: 8/10 | Loss: 0.00397808151319623\n",
            "Epoch: 8/10 | Test Accuracy: 100.0\n",
            "Epoch: 9/10 | Loss: 0.002719544805586338\n",
            "Epoch: 9/10 | Test Accuracy: 100.0\n",
            "Epoch: 10/10 | Loss: 0.0022316286340355873\n",
            "Epoch: 10/10 | Test Accuracy: 100.0\n",
            "--------------------------------------------\n",
            "------------Combination: ['D', 'A']--------------\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DeiTForImageClassification were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10 | Loss: 1.1038446426391602\n",
            "Epoch: 1/10 | Test Accuracy: 46.46787362442315\n",
            "Epoch: 2/10 | Loss: 0.04351508617401123\n",
            "Epoch: 2/10 | Test Accuracy: 59.95740149094782\n",
            "Epoch: 3/10 | Loss: 0.016177324578166008\n",
            "Epoch: 3/10 | Test Accuracy: 61.09336173233937\n",
            "Epoch: 4/10 | Loss: 0.00868083443492651\n",
            "Epoch: 4/10 | Test Accuracy: 61.69684061057863\n",
            "Epoch: 5/10 | Loss: 0.0058838496915996075\n",
            "Epoch: 5/10 | Test Accuracy: 61.803336883209084\n",
            "Epoch: 6/10 | Loss: 0.004809904377907515\n",
            "Epoch: 6/10 | Test Accuracy: 61.90983315583954\n",
            "Epoch: 7/10 | Loss: 0.004589059855788946\n",
            "Epoch: 7/10 | Test Accuracy: 62.264820731274405\n",
            "Epoch: 8/10 | Loss: 0.003907616715878248\n",
            "Epoch: 8/10 | Test Accuracy: 62.22932197373092\n",
            "Epoch: 9/10 | Loss: 0.003600620198994875\n",
            "Epoch: 9/10 | Test Accuracy: 62.37131700390486\n",
            "Epoch: 10/10 | Loss: 0.002392939757555723\n",
            "Epoch: 10/10 | Test Accuracy: 62.40681576144835\n",
            "--------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LNrCflJldy4x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}